{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 메커니즘\n",
    "# 가중치 계산\n",
    "# query vector, keys vectors 유사도 기반 내적 계산 >> attention score\n",
    "# sfotmax 함수 이용, 가중치 계산\n",
    "# keys vector에 가중평균 적용 >> context vector 생성\n",
    "# query 와 key 를 입력으로 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 나는 집에 -> 갑니다 (2차원 벡터) >> 1차원 벡터\n",
    "# 입력문장에 대한 hidden state : keys\n",
    "\n",
    "keys = np.array([[0.11, 0.12, 0.13], [0.21, 0.22, 0.23]])  # 나는, 집에\n",
    "\n",
    "\n",
    "# 현재 출력 시점에서의 hidden state # query\n",
    "\n",
    "query = np.array([0.41, 0.42, 0.43])  # 갑니다\n",
    "\n",
    "print(keys.shape, query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 메커니즘\n",
    "\n",
    "# 1. attention score 계산(유사도 반영)\n",
    "\n",
    "scores = np.dot(keys, query)\n",
    "\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46854161, 0.53145839])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Attention weights 계산(softmax) >> 중요도 계산\n",
    "\n",
    "weights = np.exp(scores) / np.sum(np.exp(scores))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46854161, 0.46854161, 0.46854161],\n",
       "       [0.53145839, 0.53145839, 0.53145839]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_repeated = weights.repeat(3).reshape(2,3)\n",
    "weights_repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vector :\n",
      " [[0.05153958 0.05622499 0.06091041]\n",
      " [0.11160626 0.11692085 0.12223543]]\n"
     ]
    }
   ],
   "source": [
    "# 가중평균 계산 \n",
    "context_vector = keys * weights_repeated\n",
    "print('context_vector :\\n', context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16314584, 0.17314584, 0.18314584])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = context_vector.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16314584, 0.17314584, 0.18314584])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀프 어텐션(Self-Attention)\n",
    "\n",
    "# 하나의 문장 또는 문서에서 단어 간 상호작용을 모델링 하기 위해 사용하는 어텐션 메커니즘\n",
    "\n",
    "# 셀프 어텐션 구현 단계\n",
    "# 1. 임베딩 층을 통해 입력 문장을 임베딩 벡터로 변환\n",
    "# 2. 입력 임베딩 벡터 >> query, key, value 구분해서 나눔 >>\n",
    "# 3. 각각을 query, keys, values 변수에 저장\n",
    "# 4. query와 key 내적, 어텐션 스코어(attention score) 계산\n",
    "# 5. 어텐션 스코어에 softmax 함수 적용 >> (전체의 합이 1인 확률값 변환) >> 가중치(weights) 계산\n",
    "# 6. 가중치(weights)와 value 곱하여 컨텍스트 벡터 (context vector) 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def self_attention(inputs) :\n",
    "    # inputs : [batch_size, seq_len, d_model]\n",
    "    \n",
    "    # query, key, value 생성\n",
    "    # 실제 q, k, v 입력값은 다름(편의상 같은 값 입력값으로 사용)\n",
    "    \n",
    "    q = inputs\n",
    "    k = inputs\n",
    "    v = inputs\n",
    "    \n",
    "    # 1. 행렬 곱(내적) >> attention score 계산\n",
    "    scores = np.matmul(q, k.transpose(0, 2, 1))\n",
    "    \n",
    "    # 2. 스케일링(scaled)작업\n",
    "    d_k = q.shape[-1]\n",
    "    scores /= np.sqrt(d_k)\n",
    "    \n",
    "    # 3. softmax 함수 적용, 가중치 계산\n",
    "    weights = np.exp(scores) / np.sum(np.exp(scores), axis = -1, keepdims = True)\n",
    "    \n",
    "    # 4. 가중치(weights)와 values 를 곱함(내적) >>  context vector 계산\n",
    "    context_vector = np.matmul(weights, v)\n",
    "    \n",
    "    return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력문장에 대한 hidden state : keys \n",
    "\n",
    "inputs = np.array([[[0.11, 0.12, 0.13],  # 나는 \n",
    "                    [0.21, 0.22, 0.23],  # 집에\n",
    "                    [0.31, 0.32, 0.33]]]) # 갑니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.11, 0.21, 0.31],\n",
       "         [0.12, 0.22, 0.32],\n",
       "         [0.13, 0.23, 0.33]]]),\n",
       " (1, 3, 3))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.transpose(0,2,1), inputs.transpose(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.11],\n",
       "         [0.21],\n",
       "         [0.31]],\n",
       " \n",
       "        [[0.12],\n",
       "         [0.22],\n",
       "         [0.32]],\n",
       " \n",
       "        [[0.13],\n",
       "         [0.23],\n",
       "         [0.33]]]),\n",
       " (3, 3, 1),\n",
       " array([[[0.11],\n",
       "         [0.21],\n",
       "         [0.31]],\n",
       " \n",
       "        [[0.12],\n",
       "         [0.22],\n",
       "         [0.32]],\n",
       " \n",
       "        [[0.13],\n",
       "         [0.23],\n",
       "         [0.33]]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.transpose(), inputs.transpose().shape, inputs.transpose(2,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vector :\n",
      " [[[0.21138554 0.22138554 0.23138554]\n",
      "  [0.21253973 0.22253973 0.23253973]\n",
      "  [0.21369315 0.22369315 0.23369315]]]\n"
     ]
    }
   ],
   "source": [
    "context_vector = self_attention(inputs)\n",
    "print('context_vector :\\n', context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head attention\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 입력문장을 임베딩 한 후, 셀프 어텐션을 수행하는 함수\n",
    "\n",
    "def multi_head_attention(embeddings) :\n",
    "    # 셀프 어텐션 수행\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "                       num_heads = 8, key_dim = 512, \n",
    "                       )(embeddings, embeddings)\n",
    "    \n",
    "    # 셀프어텐션 출력값 >> 임베딩벡터에 더함 >> 최종 출력값 생성\n",
    "    add_output = tf.keras.layers.Add()([attention_output, embeddings])\n",
    "    \n",
    "    # 출력값을 정규화\n",
    "    normalization_output = tf.keras.layers.LayerNormalization()(add_output)\n",
    "    \n",
    "    return normalization_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 10, 512, 512) dtype=float32 (created by layer 'layer_normalization')>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시 입력 문장\n",
    "inputs = tf.keras.layers.Input(shape = (10, 512))\n",
    "\n",
    "# 입력 임베딩\n",
    "embeddings = tf.keras.layers.Embedding(input_dim = 10000, output_dim = 512)(inputs)\n",
    "\n",
    "# 멀티 헤더 어텐션 수행\n",
    "attention_output = multi_head_attention(embeddings)\n",
    "attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
