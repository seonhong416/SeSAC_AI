{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "                                              0.0/474.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 474.6/474.6 kB 31.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "                                              0.0/110.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 110.5/110.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "                                              0.0/132.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 132.9/132.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (23.0)\n",
      "Collecting responses<0.19 (from datasets)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sbauser\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sbauser\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
      "Successfully installed datasets-2.12.0 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# 한국어 금융 뉴스 긍정, 부정 감성 분류\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('finance_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4846 entries, 0 to 4845\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   labels        4846 non-null   int64 \n",
      " 1   sentence      4846 non-null   object\n",
      " 2   kor_sentence  4846 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 113.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2879\n",
       "1    1363\n",
       "2     604\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()\n",
    "# 긍정, 중립, 부정 >> 3개의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>sentence</th>\n",
       "      <th>kor_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>According to Gran, the company has no plans to...</td>\n",
       "      <td>Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>According to the company's updated strategy fo...</td>\n",
       "      <td>2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>2</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "      <td>런던 마켓워치 -- 은행주의 반등이 FTSE 100지수의 약세를 상쇄하지 못하면서 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>0</td>\n",
       "      <td>Rinkuskiai's beer sales fell by 6.5 per cent t...</td>\n",
       "      <td>린쿠스키아의 맥주 판매량은 416만 리터로 6.5% 감소했으며 카우노 알루스의 맥주...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>2</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "      <td>영업이익은 2007년 68.8 mn에서 35.4 mn으로 떨어졌으며, 선박 판매 이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>2</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "      <td>페이퍼 부문 순매출은 2008년 2분기 241.1 mn에서 2009년 2분기 221...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>2</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "      <td>핀란드에서의 판매는 1월에 10.5% 감소한 반면, 국외에서의 판매는 17% 감소했다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels                                           sentence  \\\n",
       "0          0  According to Gran, the company has no plans to...   \n",
       "1          0  Technopolis plans to develop in stages an area...   \n",
       "2          2  The international electronic industry company ...   \n",
       "3          1  With the new production plant the company woul...   \n",
       "4          1  According to the company's updated strategy fo...   \n",
       "...      ...                                                ...   \n",
       "4841       2  LONDON MarketWatch -- Share prices ended lower...   \n",
       "4842       0  Rinkuskiai's beer sales fell by 6.5 per cent t...   \n",
       "4843       2  Operating profit fell to EUR 35.4 mn from EUR ...   \n",
       "4844       2  Net sales of the Paper segment decreased to EU...   \n",
       "4845       2  Sales in Finland decreased by 10.5 % in Januar...   \n",
       "\n",
       "                                           kor_sentence  \n",
       "0     Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...  \n",
       "1     테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...  \n",
       "2     국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...  \n",
       "3     새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...  \n",
       "4     2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...  \n",
       "...                                                 ...  \n",
       "4841  런던 마켓워치 -- 은행주의 반등이 FTSE 100지수의 약세를 상쇄하지 못하면서 ...  \n",
       "4842  린쿠스키아의 맥주 판매량은 416만 리터로 6.5% 감소했으며 카우노 알루스의 맥주...  \n",
       "4843  영업이익은 2007년 68.8 mn에서 35.4 mn으로 떨어졌으며, 선박 판매 이...  \n",
       "4844  페이퍼 부문 순매출은 2008년 2분기 241.1 mn에서 2009년 2분기 221...  \n",
       "4845   핀란드에서의 판매는 1월에 10.5% 감소한 반면, 국외에서의 판매는 17% 감소했다.  \n",
       "\n",
       "[4846 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 하기 위해서 [0, 1, 2]로 정수인코딩 해주기\n",
    "\n",
    "# df['labels'] = df['labels'].replace(['neutral', 'positive', 'negative'],[0,1,2]) > 이미 돼있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('finance_data.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/SBAUser/.cache/huggingface/datasets/csv/default-5b21fc165b4714fa/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/SBAUser/.cache/huggingface/datasets/csv/default-5b21fc165b4714fa/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 499.14it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = load_dataset('csv', data_files={'train' : 'finance_data.csv'})\n",
    "# 모든 데이터가 train에 저장 되어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'sentence', 'kor_sentence'],\n",
       "        num_rows: 4846\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = all_data['train'].train_test_split(0.2) # 8 : 2 train, test data 구분\n",
    "train_cs = cs['train']\n",
    "test_cs = cs['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'sentence', 'kor_sentence'],\n",
      "    num_rows: 3876\n",
      "})\n",
      "\n",
      "Dataset({\n",
      "    features: ['labels', 'sentence', 'kor_sentence'],\n",
      "    num_rows: 970\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_cs)\n",
    "print()\n",
    "print(test_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  검증 데이터 생성(훈련용 데이터셋에서 분리)\n",
    "cs = train_cs.train_test_split(0.2)\n",
    "train_cs = cs['train']\n",
    "val_cs = cs['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'sentence', 'kor_sentence'],\n",
      "    num_rows: 3100\n",
      "})\n",
      "\n",
      "Dataset({\n",
      "    features: ['labels', 'sentence', 'kor_sentence'],\n",
      "    num_rows: 970\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_cs)\n",
    "print()\n",
    "print(test_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환매는 분배 가능한 자본금과 적립금을 감소시킬 것이다.\n"
     ]
    }
   ],
   "source": [
    "# 두 번째 샘플 출력\n",
    "print(train_cs['kor_sentence'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 두 번째 샘플의 레이블 출력\n",
    "print(train_cs['labels'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 전처리\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# BERT 사용하기 위한 모듈 부르기\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# 패딩 하기 위한 모듈 부르기\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 전처리 및 평가 지표\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터, 검증데이터, 테스트 데이터에 대해 [CLS] 문장 [SEP] 구조로 만든다\n",
    "# [CLS] : BERT가 분류 위해 사용하는 첫번째 입력 토큰\n",
    "# [SEP] : 입력 문장의 종료를 나타내기 위한 스페셜 토큰\n",
    "\n",
    "train_sequences = list(train_cs['kor_sentence'])\n",
    "val_sequences = list(val_cs['kor_sentence'])\n",
    "test_sequences = list(test_cs['kor_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = list(train_cs['labels'])\n",
    "val_labels = list(val_cs['labels'])\n",
    "test_labels = list(test_cs['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SWOT 분석은 분류의 한 가지 방법일 뿐이며 그 나름의 단점이 있다.',\n",
       " '환매는 분배 가능한 자본금과 적립금을 감소시킬 것이다.',\n",
       " '지속가능발전에 대한 의지가 높은 상위 기업이 지수 안에 포함된다.',\n",
       " '그러나 시는 공원 벤치, 쓰레기통, 공중화장실, 버스 쉼터, 가로등과 같은 맞춤형 공공가구 공공조달 입찰을 유치할 예정이다.',\n",
       " '비경상항목을 제외한 영업이익은 40.6유로로 전년 동기 대비 57.3유로보다 감소했다.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:5] # 0 : 중립, 1: 긍정, 2: 부정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# BERT TOKENIZER 이용, 처리\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "# 정수 인코딩 과정\n",
    "# 각 텍스트를 토큰화 >> vocabulary에 mapping 되는 정수 시퀀스로 변환\n",
    "# [안녕하세요] >> ['안', '녕', '하세요'] >> [231, 52, 45]\n",
    "\n",
    "def data_to_tensor(sentences, labels, max_len = MAX_LEN) :\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    input_idx = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    \n",
    "    # pad_sequences\n",
    "    # 패딩을 위한 모듈\n",
    "    # >> 주어진 최대 길이를 위해서 뒤에서 0으로 채워줌\n",
    "    input_idx = pad_sequences(input_idx, maxlen=max_len, dtype = 'long', truncating = 'post', padding = 'post')\n",
    "    # truncating='post' : maxlen(최대 길이) 초과 부분은 잘라냄\n",
    "    # padding = 'post' : 0으로 채움\n",
    "    \n",
    "    attention_masks = []\n",
    "    \n",
    "    for seq in input_idx :\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    # input_idx 요소에서 0이 아닌 부분 >> 1, \n",
    "    # 0인 부분 >> 0으로 변환\n",
    "    \n",
    "    # 반환된 값, input된 idx, labels를 텐서로 변환\n",
    "    tensor_inputs = torch.tensor(input_idx)\n",
    "    tensor_labels = torch.tensor(labels)\n",
    "    tensor_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return tensor_inputs, tensor_labels, tensor_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련용 검증용 테스트용 데이터에 대해 \n",
    "# data_to_tensor 함수 사용 >> 정수 인코딩 된 데이터, 라벨, 어텐션 마스크 얻기\n",
    "\n",
    "train_inputs, train_labels, train_masks = data_to_tensor(train_sequences, train_labels)\n",
    "val_inputs, val_labels, val_masks = data_to_tensor(val_sequences, val_labels)\n",
    "test_inputs, test_labels, test_masks = data_to_tensor(test_sequences, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9815,   429,   421,   426, 10719,  8135, 10585,  8143,  9036,  9340,\n",
      "         9597,  8149,  9856,  9639,  9022, 47633, 35764, 10960,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0],\n",
      "       dtype=torch.int32)\n",
      "\n",
      "tensor(0)\n",
      "\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs[0])\n",
    "print()\n",
    "print(train_labels[0])\n",
    "print()\n",
    "print(train_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# 파이토치 데이터로더(배치 단위로 데이터 꺼내오는 모듈) 변환\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기 : 3100\n",
      "검증 데이터 크기 : 776\n",
      "테스트 데이터 크기 : 970\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터 크기 :', len(train_labels))\n",
    "print('검증 데이터 크기 :', len(val_labels))\n",
    "print('테스트 데이터 크기 :', len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "# GPU 정상 세팅 여부 확인\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "    device = torch.device('cuda')\n",
    "    print(torch.cuda.device_count()) # 몇 개 gpu 사용 가능 확인\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else :\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 로드하기 \n",
    "\n",
    "# 텍스트 분류 BERT : BertForSequenceClassfication.from_pretrained('모델 이름')\n",
    "\n",
    "num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('skt/kogpt2-base-v2', num_labels = num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBAUser\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer 선택\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed) :\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds = elapsed_rounded)) # hh : mm : ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(predictions, labels) :\n",
    "    y_pred = predictions\n",
    "    y_true = labels\n",
    "    \n",
    "    # 사용 가능한 metrics 사용\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro_avg = f1_score(y_true = y_true, y_pred=y_pred, average='macro', zero_division=0)\n",
    "    f1_micro_avg = f1_score(y_true = y_true, y_pred=y_pred, average='micro', zero_division=0)\n",
    "    f1_weighted_avg = f1_score(y_true = y_true, y_pred=y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    result = {'accuracy' : acc, 'f1_macro_average' : f1_macro_avg, 'f1_micro_average' : f1_micro_avg, 'f1_weighted_average' : f1_weighted_avg}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 1 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/97 [00:00<?, ?it/s]C:\\Users\\SBAUser\\AppData\\Local\\Temp\\ipykernel_17844\\287155217.py:31: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
      "100%|██████████| 97/97 [00:36<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.5707\n",
      "Training epoch took : 0:00:37\n",
      "----- Epoch 2 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:34<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.3066\n",
      "Training epoch took : 0:00:34\n",
      "----- Epoch 3 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:34<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.1768\n",
      "Training epoch took : 0:00:35\n",
      "----- Epoch 4 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:34<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.0863\n",
      "Training epoch took : 0:00:35\n",
      "----- Epoch 5 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:33<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.0668\n",
      "Training epoch took : 0:00:34\n",
      "----- Epoch 6 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:33<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.0297\n",
      "Training epoch took : 0:00:33\n",
      "----- Epoch 7 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:32<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.0192\n",
      "Training epoch took : 0:00:33\n",
      "----- Epoch 8 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:33<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.0064\n",
      "Training epoch took : 0:00:33\n",
      "----- Epoch 9 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:33<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.0014\n",
      "Training epoch took : 0:00:33\n",
      "----- Epoch 10 / 10 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:33<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss : 0.0001\n",
      "Training epoch took : 0:00:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "\n",
    "seed_val = 777\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "model.zero_grad()\n",
    "for epoch in range(epochs) :\n",
    "    print('----- Epoch {:} / {:} -----'.format(epoch+1, epochs))\n",
    "    t0 = time.time() # 현재 시간 >> t0 저장\n",
    "    total_loss = 0 \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for step,batch in tqdm(enumerate(train_dataloader), total = len(train_dataloader)) :\n",
    "        if step % 500 == 0 and not step ==0 :\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5, } of {:5,}. Elapsed : {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_idx, b_input_mask, b_labels = batch\n",
    "        # 각 배치를 device에 할당, b_input_idx, b_input_mask, b_labels 데이터 저장\n",
    "        \n",
    "        outputs = model(b_input_idx, attention_mask = b_input_mask, labels = b_labels)\n",
    "        \n",
    "        loss = outputs[0] # output의 첫번째 요소 >> loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "        # clip_grad_norm\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        model.zero_grad()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    print()\n",
    "    print('Average training loss : {0:.4f}'.format(avg_train_loss))\n",
    "    print('Training epoch took : {:}'.format(format_time(time.time()-t0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터에 대한 평가\n",
    "\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "# 모델 평가 모드 전환\n",
    "accum_logits, accum_label_idx = [], []\n",
    "\n",
    "for batch in val_dataloader :\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_idx, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        outpus = model(b_input_idx, token_type_ids = None, attention_mask = b_input_mask)\n",
    "        # token_type_ids = None : 하나의 문장으로만 이루어진 경우에 설정\n",
    "        # BERT 모델 사용 시, 입력 데이터가 2개 문장으로 이루어진 경우 사용\n",
    "        # >> 두 문장 사이 경계 구분할 때 사용\n",
    "        # token_type_ids = None 경계가 없기 때문에 None으로 설정\n",
    "        \n",
    "        \n",
    "        logits = outpus[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_idx = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        for b in logits :\n",
    "            accum_logits.append(np.argmax(b))\n",
    "            \n",
    "        for b in label_idx :\n",
    "            accum_label_idx.append(b)\n",
    "    \n",
    "accum_logits = np.array(accum_logits)\n",
    "accum_label_idx = np.array(accum_label_idx)\n",
    "result = metrics(accum_logits, accum_label_idx)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8492\n",
      "f1_macro_score : 0.8403\n",
      "f1_micro_score : 0.8492\n",
      "f1_weighted_score : 0.8496\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy : {0:.4f}'.format(result['accuracy']))\n",
    "print('f1_macro_score : {0:.4f}'.format(result['f1_macro_average']))\n",
    "print('f1_micro_score : {0:.4f}'.format(result['f1_micro_average']))\n",
    "print('f1_weighted_score : {0:.4f}'.format(result['f1_weighted_average']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장과 로드\n",
    "\n",
    "torch.save(model.state_dict(), './GPT_news_sentiment_analysis_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./GPT_news_sentiment_analysis_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:03,  8.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에 대한 평가\n",
    "\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "# 모델 평가 모드 전환\n",
    "accum_logits, accum_label_idx = [], []\n",
    "\n",
    "for step, batch in tqdm(enumerate(test_dataloader)) :\n",
    "    if step % 100 == 0 and not step == 0 :\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('Batch {:>5,} of {:>,}. Elapsed : {:}'.format(step, len(test_dataloader), elapsed))\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_idx, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad() :\n",
    "        outpus = model(b_input_idx, token_type_ids = None, attention_mask = b_input_mask)\n",
    "        # token_type_ids = None : 하나의 문장으로만 이루어진 경우에 설정\n",
    "        # BERT 모델 사용 시, 입력 데이터가 2개 문장으로 이루어진 경우 사용\n",
    "        # >> 두 문장 사이 경계 구분할 때 사용\n",
    "        # token_type_ids = None 경계가 없기 때문에 None으로 설정\n",
    "        \n",
    "        \n",
    "    logits = outpus[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_idx = b_labels.to('cpu').numpy()\n",
    "        \n",
    "    for b in logits :\n",
    "        accum_logits.append(np.argmax(b))\n",
    "            \n",
    "    for b in label_idx :\n",
    "        accum_label_idx.append(b)\n",
    "    \n",
    "accum_logits = np.array(accum_logits)\n",
    "accum_label_idx = np.array(accum_label_idx)\n",
    "result = metrics(accum_logits, accum_label_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9597938144329897,\n",
       " 'f1_macro_average': 0.9532493112947659,\n",
       " 'f1_micro_average': 0.9597938144329897,\n",
       " 'f1_weighted_average': 0.9599476697622902}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SBAUser\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('text-classification', model=model.cuda(), tokenizer = tokenizer, device = 0, max_length=512, return_all_scores = False, function_to_apply='softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9999961853027344}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pipe('주식회사 한시경 매출이 급성장했다네요')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'LABEL_0' : '중립', 'LABEL_1' : '긍정', 'LABEL_2' : '부정'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text) :\n",
    "    result = pipe(text)\n",
    "    \n",
    "    return label_dict[result[0]['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'긍정'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction('서울시 세싹 강동캠퍼스 교육생 전원 취업했대')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'부정'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction('배가 불러서 너무 졸려요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'중립'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction('아버지가 방에 들어가셨다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
