{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 훈련된 임베딩을 사용하지 않는 경우\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성 분류 모델(긍정:1, 부정:0)\n",
    "\n",
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화된 결과 : [['nice', 'great', 'best', 'amazing'], ['stop', 'lies'], ['pitiful', 'nerd'], ['excellent', 'work'], ['supreme', 'quality'], ['bad'], ['highly', 'respectable']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [sent.split() for sent in sentences]\n",
    "print('단어 토큰화된 결과 :', tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nice',\n",
       " 'great',\n",
       " 'best',\n",
       " 'amazing',\n",
       " 'stop',\n",
       " 'lies',\n",
       " 'pitiful',\n",
       " 'nerd',\n",
       " 'excellent',\n",
       " 'work',\n",
       " 'supreme',\n",
       " 'quality',\n",
       " 'bad',\n",
       " 'highly',\n",
       " 'respectable']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = []\n",
    "\n",
    "for sent in tokenized_sentences :\n",
    "    for word in sent :\n",
    "        word_list.append(word)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어수 : 15\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter(word_list)\n",
    "print('총 단어수 :', len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nice',\n",
       " 'great',\n",
       " 'best',\n",
       " 'amazing',\n",
       " 'stop',\n",
       " 'lies',\n",
       " 'pitiful',\n",
       " 'nerd',\n",
       " 'excellent',\n",
       " 'work',\n",
       " 'supreme',\n",
       " 'quality',\n",
       " 'bad',\n",
       " 'highly',\n",
       " 'respectable']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 등장 빈도순으로 정렬\n",
    "\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<UNK>'] = 1\n",
    "\n",
    "for index, word in enumerate(vocab) :\n",
    "    word_to_index[word] = index + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 토큰, UNK 토큰 고려한 단어 집합 크기 : 17\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "print('패딩 토큰, UNK 토큰 고려한 단어 집합 크기 :', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(tokenized_x_data, word_to_index) :\n",
    "    encoded_x_data = []\n",
    "    \n",
    "    for sent in tokenized_x_data :\n",
    "        index_sequences = []\n",
    "        for word in sent :\n",
    "            try :\n",
    "                index_sequences.append(word_to_index[word])\n",
    "            \n",
    "            except KeyError :\n",
    "                index_sequences.append(word_to_index['<UNK>'])\n",
    "    \n",
    "        encoded_x_data.append(index_sequences)\n",
    "    \n",
    "    return encoded_x_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoded = texts_to_sequences(tokenized_sentences, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(i) for i in x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대길이 : 4\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(i) for i in x_encoded)\n",
    "print('최대길이 :', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sentences, max_len) :\n",
    "    features = np.zeros((len(sentences), max_len), dtype = int)\n",
    "    \n",
    "    for index, sentence in enumerate(sentences) :\n",
    "        if len(sentence) != 0 :\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "            \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5],\n",
       "       [ 6,  7,  0,  0],\n",
       "       [ 8,  9,  0,  0],\n",
       "       [10, 11,  0,  0],\n",
       "       [12, 13,  0,  0],\n",
       "       [14,  0,  0,  0],\n",
       "       [15, 16,  0,  0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(x_encoded, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 결과 :\n",
      "[[ 2  3  4  5]\n",
      " [ 6  7  0  0]\n",
      " [ 8  9  0  0]\n",
      " [10 11  0  0]\n",
      " [12 13  0  0]\n",
      " [14  0  0  0]\n",
      " [15 16  0  0]]\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(x_encoded, max_len = max_len)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print('패딩 결과 :')\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim) :\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_dim*max_len, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        # embedding.shape == (배치 크기, 문장의 길이, 임베딩 벡터 차원)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # flattend.shape == (배치크기* 문장의 길이, 임베딩 벡터 차원)\n",
    "        flattend = self.flatten(embedded)\n",
    "        \n",
    "        #output.shape == (배치크기, 1)\n",
    "        output = self.fc(flattend)\n",
    "        \n",
    "        return self.sigmoid(output)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = SimpleModel(vocab_size, 100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "# BCELOSS : 이진분류(binary classification)\n",
    "\n",
    "optimizer = Adam(simple_model.parameters())\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(x_train, dtype = torch.long), \n",
    "                              torch.tensor(y_train, dtype = torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t Loss : 0.5270470380783081\n",
      "Epoch : 2 \t Loss : 0.5010523200035095\n",
      "Epoch : 3 \t Loss : 0.44444406032562256\n",
      "Epoch : 4 \t Loss : 0.3799958825111389\n",
      "Epoch : 5 \t Loss : 0.3184640407562256\n",
      "Epoch : 6 \t Loss : 0.26483941078186035\n",
      "Epoch : 7 \t Loss : 0.22059763967990875\n",
      "Epoch : 8 \t Loss : 0.18523289263248444\n",
      "Epoch : 9 \t Loss : 0.1573697179555893\n",
      "Epoch : 10 \t Loss : 0.1354503184556961\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10) :\n",
    "    for inputs, targets in train_dataloader :\n",
    "        # inputs.shape = (배치 크기, 문장 길이)\n",
    "        # targets.shape = (배츠 크기)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # outputs.shape == (배치 크기)\n",
    "        outputs = simple_model(inputs).view(-1)\n",
    "        # view(-1) : 반환된 출력값을 1차원 배열로 변환\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch : {epoch+1} \\t Loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 훈련된 임베딩을 사용할 경우\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j&#39; -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j\" -O GoogleNews-vectors-negative300.bin.gz && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글의 사전 훈련된 word2vec 모델 로드\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 행렬 크기 : (17, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "print('임베딩 행렬 크기 :', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word) :\n",
    "    if word in word2vec_model :\n",
    "        return word2vec_model[word]\n",
    "    else :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_to_index.items() :\n",
    "    if i > 2 :\n",
    "        temp = get_vector(word)\n",
    "        if temp is not None :\n",
    "            embedding_matrix[i] = temp\n",
    "\n",
    "# if i > 2 : <PAD>, <UNK> 0, 1번은 실제 단어가 아님 >> mapping 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec_model에서 'greate'의 임베딩 벡터\n",
    "# embedding_matrix[3] 이 일치하는 지 확인\n",
    "\n",
    "np.all(word2vec_model['great'] == embedding_matrix[3])\n",
    "# np.all() : 모든 원소가 참(True)인지 검사하는 함수(boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedEmbeddingModel(nn.Module) :\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim) :\n",
    "        super(PreTrainedEmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype = torch.float32))\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_dim * max_len, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        embedded = self.embedding(x) \n",
    "        flattened = self.flatten(embedded)\n",
    "        output = self.fc(flattened)\n",
    "        return self.sigmoid(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding_model = PreTrainedEmbeddingModel(vocab_size, 300).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedEmbeddingModel(\n",
       "  (embedding): Embedding(17, 300)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=1200, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(pretrained_embedding_model.parameters())\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(x_train, dtype = torch.long), \n",
    "                              torch.tensor(y_train, dtype = torch.float32))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 \t Loss : 0.695076048374176\n",
      "Epoch : 2 \t Loss : 0.6320176720619202\n",
      "Epoch : 3 \t Loss : 0.5693480968475342\n",
      "Epoch : 4 \t Loss : 0.5104113817214966\n",
      "Epoch : 5 \t Loss : 0.4561007022857666\n",
      "Epoch : 6 \t Loss : 0.4066222310066223\n",
      "Epoch : 7 \t Loss : 0.36190178990364075\n",
      "Epoch : 8 \t Loss : 0.3217328190803528\n",
      "Epoch : 9 \t Loss : 0.28584054112434387\n",
      "Epoch : 10 \t Loss : 0.2539149224758148\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10) :\n",
    "    for inputs, targets in train_dataloader :\n",
    "        # inputs.shape == (배치 크기, 문장 길이)\n",
    "        # targets.shape == (배치 크기)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # outputs.shape == (배치 크기)\n",
    "        outputs = pretrained_embedding_model(inputs).view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch : {epoch +1} \\t Loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
