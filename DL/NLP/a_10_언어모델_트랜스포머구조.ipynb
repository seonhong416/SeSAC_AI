{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"tsQCC7plUF2V","executionInfo":{"status":"ok","timestamp":1683591730355,"user_tz":-540,"elapsed":11804,"user":{"displayName":"주식회사한시경","userId":"01036671795204052858"}}},"outputs":[],"source":["from torch import nn\n","\n","# Transformer 모델 정의\n","class Transformer(nn.Module):\n","\n","  def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512,\n","               nhead=8, num_encoder_layers=6, num_decoder_layers=6,\n","               dim_feedforward=2048, dropout=0.1):\n","    super(Transformer, self).__init__()\n","\n","    # 인코더와 디코더에 사용할 임베딩 레이어 정의\n","    self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n","    self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","    # src : source, tgr : target\n","\n","    # 포지셔널 인코딩 레이어 정의\n","    self.positional_encoding = PositionalEncoding(d_model)\n","\n","    # 트랜스포머 인코더 레이어 정의\n","    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n","                                               nhead=nhead,\n","                                               dim_feedforward=dim_feedforward,\n","                                               dropout=dropout)\n","    \n","    encoder_norm = nn.LayerNorm(d_model)\n","    \n","    self.encoder = nn.TransformerEncoder(encoder_layer,\n","                                         num_layers=num_encoder_layers,\n","                                         norm=encoder_norm)\n","    \n","    # 트랜스포머 디코더 레이어 정의\n","    decoder_layer = nn.TransformerDecoderLayer(d_model=d_model,\n","                                               nhead=nhead,\n","                                               dim_feedforward=dim_feedforward,\n","                                               dropout=dropout)\n","    decoder_norm = nn.LayerNorm(d_model)\n","    \n","    self.decoder = nn.TransformerDecoder(decoder_layer,\n","                                         num_layers=num_decoder_layers,\n","                                         norm=decoder_norm)\n","\n","    # 출력층 정의\n","    self.out = nn.Linear(d_model, tgt_vocab_size)\n","\n","\n","\n","\n","  def forward(self, src, tgt):\n","        \n","    # 입력과 출력 문장에 임베딩 적용\n","    src_emb = self.src_embedding(src) * math.sqrt(self.d_model) \n","    tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model) \n","\n","\n","    # 입력과 출력 문장에 포지셔널 인코딩 적용\n","    src_pos_emb = self.positional_encoding(src_emb)\n","    tgt_pos_emb = self.positional_encoding(tgt_emb)\n","\n","    \n","    # 마스크 생성\n","    src_key_padding_mask = (src == PAD_IDX).transpose(0, 1) \n","    tgt_key_padding_mask = (tgt == PAD_IDX).transpose(0, 1) \n","    memory_key_padding_mask = (src == PAD_IDX).transpose(0, 1) \n","    tgt_mask = generate_square_subsequent_mask(tgt.size(0)).to(device)\n","    # generate_square_subsequent_mask() \n","    # 특정 sequence 길이를 가지는 마스크 생성 함수 \n","\n","    # 인코더에 소스 문장을 입력하여 메모리 생성\n","    memory = self.encoder(src_pos_emb.transpose(0, 1),\n","                          mask=None,\n","                          src_key_padding_mask=src_key_padding_mask)\n","\n","    # 디코더에 메모리와 타겟 문장을 입력하여 출력 생성\n","    output = self.decoder(tgt_pos_emb.transpose(0 ,1),\n","                          memory.transpose(0 ,1),\n","                          tgt_mask=tgt_mask,\n","                          memory_mask=None,\n","                          tgt_key_padding_mask=tgt_key_padding_mask,\n","                          memory_key_padding_mask=memory_key_padding_mask)\n","\n","    # 출력을 선형 변환하여 최종 예측 생성\n","    output_logits=self.out(output.transpose(0 ,1))\n","\n","    return output_logits"]},{"cell_type":"code","source":[],"metadata":{"id":"-RqkDiqeUIkY"},"execution_count":null,"outputs":[]}]}