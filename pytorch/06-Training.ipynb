{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6y8AwKGB8-Mo"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"Q_S-IHD88-My"},"source":["# Training a Neural Network\n","\n","## Softmax 층\n","\n","$$y_i = \\text{Softmax}(z_i) = \\dfrac{\\exp(z_i)}{\\sum_j^k \\exp(z_j)}$$\n","\n","`torch.softmax` 를 사용하면 함수처럼 사용할 수 있고, `nn.Softmax()`를 사용해서 객체 생성 후 하나의 층 처럼 사용할 수도 있다. 다만 Softmax 할 차원을 지정해줘야한다.\n","\n","* PyTorch Docs: [softmax](https://pytorch.org/docs/stable/nn.html#softmax)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":1132,"status":"ok","timestamp":1586376616589,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"TVQIuRac8-Mz","outputId":"6c9b7cd2-bd9f-4597-d227-da772743c88f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.002 0.    0.018 0.979]\n"]}],"source":["x = torch.FloatTensor([3, 1, 5, 9])\n","prob = torch.softmax(x, dim=0)\n","print(f\"{prob.numpy().round(3)}\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":1120,"status":"ok","timestamp":1586376616589,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"NBNosEXf8-M4","outputId":"1cbbb131-6eb4-44f1-fe8a-a20a07a8df4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 4])\n","[[0.002 0.    0.018 0.979]\n"," [0.087 0.644 0.237 0.032]]\n","\n","dim=1 으로 합을 하면 1이 된다: tensor([1.0000, 1.0000])\n"]}],"source":["softmax_layer = nn.Softmax(dim=1)\n","x = torch.FloatTensor([[3, 1, 5, 9], \n","                       [4, 6, 5, 3]])\n","print(x.size())\n","prob = softmax_layer(x)\n","print(f\"{prob.numpy().round(3)}\\n\")\n","print(f\"dim=1 으로 합을 하면 1이 된다: {prob.sum(1)}\")"]},{"cell_type":"markdown","metadata":{"id":"b0yOAcOq8-M8"},"source":["보통 softmax의 0에 가까운 아주 작은 수라서 log를 취해서 `torch.log_softmax`를 사용한다(혹은 `nn.LogSoftmax`)."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":1111,"status":"ok","timestamp":1586376616590,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"77lk2P_A8-M-","outputId":"6241947f-23b6-4280-b95c-e497d06f27f9"},"outputs":[{"data":{"text/plain":["tensor([[-6.0209, -8.0209, -4.0209, -0.0209],\n","        [-2.4402, -0.4402, -1.4402, -3.4402]])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["torch.log_softmax(x, dim=1)"]},{"cell_type":"markdown","metadata":{"id":"OWG4Ba_w8-NA"},"source":["## Loss Function\n","\n","XOR 문제는 0과 1 두 가지 클래스를 분류하는 문제다. Cross-Entropy 를 사용할 수 있다. \n","\n","따라서 네트워크의 마지막 층을 2개로 출력하고 `Softmax` 층을 넣어야 한다. 다만 PyTorch의 Cross Entropy Loss(`nn.CrossEntropyLoss`)에는 `LogSoftmax`가 포함되어 있어서 넣지 않아도 된다."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"lH9rDNAO8-NB"},"outputs":[],"source":["torch.manual_seed(70)\n","\n","class XOR(nn.Module):\n","    \"\"\"XOR Network\"\"\"\n","    def __init__(self):\n","        super(XOR, self).__init__()\n","        # 층을 구성\n","        self.layers = nn.Sequential(\n","            nn.Linear(2,3),  # in_features, out_features\n","            nn.Sigmoid(),\n","            nn.Linear(3,2),\n","            nn.Sigmoid(),\n","            nn.Linear(2,2)\n","        )\n","\n","    def forward(self, x):\n","        # forward propagation 수행\n","        o = self.layers(x)\n","        return o\n","    \n","    def predict(self, x):\n","        o = self.forward(x)\n","        y = torch.softmax(o, dim=1)\n","        return y"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":1098,"status":"ok","timestamp":1586376616591,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"-6oQLEtT8-ND","outputId":"2391915d-bab9-4418-cfb0-a68b1df178bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss value: 0.2911781370639801\n"]}],"source":["# 입력텐서 타겟 텐서 생성    \n","x = torch.Tensor([[0, 1]])\n","t = torch.LongTensor([0])\n","\n","# 커스텀 모듈 호출\n","model = XOR()\n","\n","# 손실함수\n","loss_function = nn.CrossEntropyLoss()\n","\n","# 순방향전파\n","y = model(x)\n","\n","# 손실값 계산\n","loss = loss_function(y, t)\n","\n","print(f\"loss value: {loss.item()}\")"]},{"cell_type":"markdown","metadata":{"id":"JbK-35a18-NE"},"source":["## nn.AutoGrad\n","\n","PyTorch의 AutoGrad는 Tensor의 미분 자동화를 돕는 패키지다. 각 텐서에는 `requires_grad`라는 속성이 있어서 미분이 필요한 텐서인지 아닌지 확인할 수 있다. 또한 `requires_grad_` 함수를 호출하면 해당 텐서는 미분이 필요한 텐서가 되며, 역전파시 미분을 계산하게 된다.\n","\n","* 함수뒤에 \"\\_\" 표시는 in-place operations 으로써 실행하면 새로운 메모리에 할당하지 않고, 메모리를 차지하고 있는 텐서에 덮어쓰는 형식이다. PyTorch에서는 사용을 권장하고 있지 않다. [관련 링크](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":1087,"status":"ok","timestamp":1586376616592,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"-2oldEG48-NF","outputId":"fe67ce85-1d2c-4632-8fa3-bf5e5fdbca9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["require gradient? False\n","tensor([10.])\n","\n","require gradient? True\n","tensor([10.], requires_grad=True)\n"]}],"source":["x = torch.FloatTensor([10])\n","\n","print(f\"require gradient? {x.requires_grad}\")\n","print(x)\n","print()\n","\n","# requires_grad\n","x.requires_grad_(True)\n","# '_' x의 특성을 바꾼다.\n","print(f\"require gradient? {x.requires_grad}\")\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"7qNCzQ058-NG"},"source":["<img src=\"https://drive.google.com/uc?id=1SPGm636Na_VrRTHkcBhMOQGMq0CaYAtg\" width=\"640px\" >\n","\n","예제로 계산 그래프를 그려본다.\n","\n","$$\\begin{aligned}\n","c(a, b) &= a + b\\\\\n","d(b) &= 2\\times b + 1\\\\\n","e(c, d) &= c\\times d \n","\\end{aligned} \\\\ \\ \\\\ \\text{where } a=2, b=3$$\n","\n","연산 경로에 미분이 필요한 텐서가 들어가면 자동으로 `requires_grad=True`가 된고, 연산이 진행된 텐서는 `grad_fn` 역전파 함수를 내포하고 있다."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":1076,"status":"ok","timestamp":1586376616592,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"SBnrpYvl8-NH","outputId":"86f8c2ee-0564-42c3-8917-c1e134ceed51"},"outputs":[{"name":"stdout","output_type":"stream","text":["require gradient?\n","  - a(=2.0): True \t/ grad_fn: \t None\n","  - b(=3.0): False \t/ grad_fn: \t None\n","  - c(=5.0): True \t/ grad_fn: \t <AddBackward0 object at 0x0000020928071910>\n","  - d(=7.0): False \t/ grad_fn: \t None\n","  - e(=35.0): True \t/ grad_fn: \t <MulBackward0 object at 0x0000020928071910>\n"]}],"source":["a = torch.FloatTensor([2]).requires_grad_()\n","b = torch.FloatTensor([3])\n","c = a + b\n","d = 2 * b + 1\n","e = c * d\n","\n","print(f\"require gradient?\")\n","for t, name in zip([a, b, c, d, e], [\"a\", \"b\", \"c\", \"d\", \"e\"]):\n","    print(f\"  - {name}(={t.item()}): {t.requires_grad} \\t/ grad_fn: \\t {t.grad_fn}\")"]},{"cell_type":"markdown","metadata":{"id":"QwNOexID8-NI"},"source":["미분을 구하려면 `backward` 함수에 경사를 전달하면 된다. 각 텐서에서 `.grad` 속성을 조회하면 미분값을 확인할 수 있다. "]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":1068,"status":"ok","timestamp":1586376616593,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"6GGcLcCv8-NJ","outputId":"ceec056e-ac51-4fe9-c66d-d3f7f2e4b456"},"outputs":[{"name":"stdout","output_type":"stream","text":["gradient\n","  - a: tensor([7.])\n","  - b: None\n","  - c: None\n","  - d: None\n","  - e: None\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\SBAUser\\AppData\\Local\\Temp\\ipykernel_1620\\4124522061.py:6: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n","  print(f\"  - {name}: {t.grad}\")\n"]}],"source":["gradient = torch.FloatTensor([1.])\n","e.backward(gradient)\n","\n","print(f\"gradient\")\n","for t, name in zip([a, b, c, d, e], [\"a\", \"b\", \"c\", \"d\", \"e\"]):\n","    print(f\"  - {name}: {t.grad}\")"]},{"cell_type":"markdown","metadata":{"id":"CPOw-uLj8-NK"},"source":["## torch.optim\n","\n","PyTorch의 최적화 관련된 것은 모두 optim 패키지에 있다. `model.parameters()`는 모델 안에 내포되있는 모든 파라미터를 `generator` 객체를 생성한다. 이를 옵티마이저에게 전달하여 업데이트할 매개변수를 등록한다. "]},{"cell_type":"code","execution_count":22,"metadata":{"id":"WFECmBIY8-NL"},"outputs":[],"source":["import torch.optim as optim\n","\n","# 입력텐서 타겟 텐서 생성    \n","x = torch.Tensor([[0, 1]])\n","t = torch.LongTensor([0])\n","\n","# 커스텀 모듈 호출\n","model = XOR()\n","\n","# 손실함수\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.1) #최적의 optimizer를 찾아가는 방법\n","\n","# 순방향전파\n","y = model(x)\n","\n","# 손실값 계산\n","loss = loss_function(y, t)\n","\n","# 역전파\n","loss.backward()"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"elapsed":1050,"status":"ok","timestamp":1586376616594,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"zghElbf9Ojws","outputId":"c78598d7-ddf6-4b8c-ecc0-8edf90cc716e"},"outputs":[{"data":{"text/plain":["[Parameter containing:\n"," tensor([[ 0.5068, -0.1696],\n","         [-0.6171, -0.1940],\n","         [-0.6448, -0.4407]], requires_grad=True),\n"," Parameter containing:\n"," tensor([ 0.6366, -0.3092, -0.1563], requires_grad=True),\n"," Parameter containing:\n"," tensor([[-0.1293, -0.3335,  0.3731],\n","         [-0.1220, -0.4077, -0.4230]], requires_grad=True),\n"," Parameter containing:\n"," tensor([-0.2977, -0.1532], requires_grad=True),\n"," Parameter containing:\n"," tensor([[ 0.1535,  0.5105],\n","         [-0.5054,  0.1989]], requires_grad=True),\n"," Parameter containing:\n"," tensor([-0.2185, -0.6025], requires_grad=True)]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["list(model.parameters())"]},{"cell_type":"markdown","metadata":{"id":"RhLKQW5N8-NM"},"source":["옵티마이저의 `.step()` 함수를 호출하면 옵티마이저가 해당 매개변수를 업데이트 해준다."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"elapsed":1371,"status":"ok","timestamp":1586376616925,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"8gBkbaoZ8-NN","outputId":"0fef625d-b015-4d3c-8fd6-1c2d3bcadf57"},"outputs":[{"name":"stdout","output_type":"stream","text":["첫번째 Linear Layer Weight: \n","Parameter containing:\n","tensor([[ 0.5068, -0.1696],\n","        [-0.6171, -0.1940],\n","        [-0.6448, -0.4407]], requires_grad=True)\n","\n","첫번째 Linear Layer Weight의 Gradient: \n","tensor([[ 0.0000,  0.0022],\n","        [ 0.0000,  0.0062],\n","        [-0.0000, -0.0021]])\n","\n","첫번째 Linear Layer Weight의 Gradient: \n","Parameter containing:\n","tensor([[ 0.5068, -0.1698],\n","        [-0.6171, -0.1946],\n","        [-0.6448, -0.4405]], requires_grad=True)\n"]}],"source":["print(\"첫번째 Linear Layer Weight: \")\n","print(model.layers[0].weight)\n","print()\n","print(\"첫번째 Linear Layer Weight의 Gradient: \")\n","print(model.layers[0].weight.grad)\n","print()\n","\n","optimizer.step()\n","\n","print(\"첫번째 Linear Layer Weight의 Gradient: \")\n","print(model.layers[0].weight)"]},{"cell_type":"markdown","metadata":{"id":"8iUMmtfE8-NP"},"source":["경사하강법 $$ W^{(1)}_{new} = W^{(1)}_{old} - \\alpha \\dfrac{\\partial L}{\\partial W^{(1)}}_{old}$$"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":1362,"status":"ok","timestamp":1586376616927,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"KAbvAt1G8-NP","outputId":"7f6b8a1d-79f9-40a8-f9ea-7245d72e1681"},"outputs":[{"name":"stdout","output_type":"stream","text":[" w_22 의 파라미터 업데이트: w = 0.3139\n"]}],"source":["print(f\" w_22 의 파라미터 업데이트: w ={0.3146 - 0.1 * (0.0074): .4f}\")"]},{"cell_type":"markdown","metadata":{"id":"9icvchVQ8-NR"},"source":["## XOR 문제 학습하기"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":12614,"status":"ok","timestamp":1586376996817,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"OTGvBbL-8-NR","outputId":"ee0e5d2e-bee3-4af5-a91f-745a844fe1f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1] Loss: 0.8337\n","[1001] Loss: 0.6932\n","[2001] Loss: 0.6932\n","[3001] Loss: 0.6931\n","[4001] Loss: 0.6931\n","[5001] Loss: 0.6931\n","[6001] Loss: 0.6931\n","[7001] Loss: 0.6927\n","[8001] Loss: 0.0309\n","[9001] Loss: 0.0032\n"]}],"source":["torch.manual_seed(70)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # gpu 사용 여부\n","n_step = 10000  # 총 학습 스텝\n","\n","# Data 세트 만들기\n","inputs = torch.FloatTensor([[0, 0], [1, 0], [0, 1], [1, 1]])\n","targets = torch.LongTensor([0, 1, 1, 0])\n","\n","# 모델 생성: gpu를 사용하려면 모델에도 device를 전달해준다.\n","model = XOR().to(device)\n","# 손실함수 정의\n","loss_function = nn.CrossEntropyLoss()\n","# 옵티마이저 정의\n","optimizer = optim.SGD(model.parameters(), lr=0.7)\n","\n","# GPU를 사용하려면 입력 텐서에도 device를 전달해준다.\n","inputs, targets = inputs.to(device), targets.to(device)\n","\n","best_loss = 999\n","# n_step 동안 학습을 진행한다.\n","for step in range(n_step):\n","    # -- 훈련단계 --\n","    train_loss = 0\n","    \n","    # 매개변수 텐서의 grad 정보를 0으로 만든다. model.zero_grad() 로도 가능하다.\n","    optimizer.zero_grad()\n","    \n","    # 순방향전파(Forward Propagation)\n","    outputs = model(inputs)\n","    \n","    # Loss 계산\n","    loss = loss_function(outputs, targets)\n","    \n","    # 역방향전파(Back Propagation)\n","    loss.backward()\n","    \n","    # 옵티마이저로 매개변수 업데이트\n","    optimizer.step()\n","    \n","    # 훈련단계 손실값 기록(모든 데이터에 손실값의 평균을 합친다.)\n","    train_loss += loss.item()\n","    if train_loss < best_loss:\n","        best_loss = train_loss\n","        torch.save(model.state_dict(), \"./xor.pt\")\n","    if step % 1000 == 0:\n","        print(f\"[{step+1}] Loss: {train_loss:.4f}\")"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"elapsed":922,"status":"ok","timestamp":1586377005283,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"Z_2N_ZVORfXq","outputId":"0c730afe-80f8-4db5-874c-c24609bf5fb8"},"outputs":[{"data":{"text/plain":["OrderedDict([('layers.0.weight',\n","              tensor([[ 1.2730,  0.5956],\n","                      [-3.1124, -3.5019],\n","                      [-5.5307, -5.3661]], device='cuda:0')),\n","             ('layers.0.bias',\n","              tensor([-0.6217,  4.9397,  2.0589], device='cuda:0')),\n","             ('layers.2.weight',\n","              tensor([[-0.5659,  1.2677, -1.8566],\n","                      [-2.8928,  6.1726, -7.4217]], device='cuda:0')),\n","             ('layers.2.bias', tensor([-0.6035, -1.0970], device='cuda:0')),\n","             ('layers.4.weight',\n","              tensor([[-1.1128, -6.9834],\n","                      [ 0.8168,  8.1486]], device='cuda:0')),\n","             ('layers.4.bias', tensor([ 3.9441, -4.0052], device='cuda:0'))])"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["model.state_dict()"]},{"cell_type":"markdown","metadata":{"id":"89BO5I8k8-NT"},"source":["## 모델 사용하기"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"elapsed":865,"status":"ok","timestamp":1586377007224,"user":{"displayName":"Sanghyun Seo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiyjIj6kA6151_HBrCZmfFqMIMsXmdFXFLAzkj8=s64","userId":"05995424227728546557"},"user_tz":-540},"id":"Jm5A45UJ8-NT","outputId":"9989ff37-75d2-496d-f14a-76008d96522f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.9984, 0.0016],\n","        [0.0015, 0.9985],\n","        [0.0013, 0.9987],\n","        [0.9977, 0.0023]], grad_fn=<SoftmaxBackward0>)\n","tensor([0, 1, 1, 0])\n","prob: tensor([0.9984, 0.0016])\t predict 0\n","prob: tensor([0.0015, 0.9985])\t predict 1\n","prob: tensor([0.0013, 0.9987])\t predict 1\n","prob: tensor([0.9977, 0.0023])\t predict 0\n"]}],"source":["# 모델 새로 정의\n","model = XOR()\n","# 모델 불러오기\n","model.load_state_dict(torch.load(\"./xor.pt\", map_location=\"cuda\"))\n","\n","probs = model.predict(inputs.cpu())\n","print(probs)\n","predicts = probs.argmax(1)\n","print(predicts)\n","for prob, pred in zip(probs, predicts):\n","    print(f\"prob: {prob.data}\\t predict {pred}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9qRsgUhRrCj"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Jyk8dDTEdfHI6oMBO1IU_PnhQZCbEfhs","timestamp":1674712118226}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
