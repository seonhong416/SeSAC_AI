{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BeautifulSoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "     -------------------------------------- 128.2/128.2 KB 7.4 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: soupsieve, BeautifulSoup4\n",
      "Successfully installed BeautifulSoup4-4.11.1 soupsieve-2.3.2.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Y\\python 3.9.13\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "     ------------------------------------- 242.1/242.1 KB 14.5 MB/s eta 0:00:00\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Y\\python 3.9.13\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ks2\\AppData\\Local\\Temp\\ipykernel_10888\\3208963346.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('chromedriver.exe')\n"
     ]
    }
   ],
   "source": [
    "# 크롬 드라이버 실행\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# url 접속하기\n",
    "url = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메가박스 성공!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ks2\\AppData\\Local\\Temp\\ipykernel_10888\\3208963346.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('chromedriver.exe')\n"
     ]
    }
   ],
   "source": [
    "# 크롬 드라이버 실행\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# url 접속하기\n",
    "url = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 불러와서 bs4로 읽기\n",
    "\n",
    "\n",
    "html = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인마가있으면 페이지가 들어가져서 xpath가 이미 로딩된상태라 안먹네  \n",
    "# 바로 밑에꺼로 다이렉트로 들어가야 되네 \n",
    "\n",
    "# driver.find_element(By.XPATH,f'//*[@id=\"movieList\"]/li[1]/div[1]/div[3]/a').send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이것저것 끼웠다가 뺐다가... \n",
    "\n",
    "\n",
    "# driver.find_element(By.XPATH,f'//*[@id=\"movieList\"]/li[{i}]/div[1]/div[3]/a').send_keys(Keys.PAGE_DOWN)\n",
    "# time.sleep(2)\n",
    "\n",
    "# driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "# time.sleep(2)\n",
    "\n",
    "#   html = driver.page_source\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래꺼로만 하면 20번째 꺼에서 멈춘다. 아마 더보기버튼 수행해주는게 필요할듯\n",
    "# 유레카 유레카 if문으로 돌리니까 해결되네 다행이다 \n",
    "# 그리고 91이 아니라 92네. 마지막꺼는 포함안되잖아? 91로하니까 90번째인 콜제인에서 꺼지는고만\n",
    "for i in range(1, 92) :    \n",
    "    driver.find_element(By.XPATH,f'//*[@id=\"movieList\"]/li[{i}]/div[1]/div[3]/a').send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    driver.back()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    if i == 20:\n",
    "        driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "        time.sleep(2)\n",
    "    elif i == 40:\n",
    "        driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "        time.sleep(2)\n",
    "    elif i == 60:\n",
    "        driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "        time.sleep(2)\n",
    "    elif i == 80:\n",
    "        driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['더 퍼스트 슬램덩크',\n",
       " '아바타: 물의 길',\n",
       " '교섭',\n",
       " '상견니',\n",
       " '유령',\n",
       " '영웅',\n",
       " '장화신은 고양이: 끝내주는 모험',\n",
       " '메간',\n",
       " '오늘 밤, 세계에서 이 사랑이 사라진다 해도',\n",
       " '바빌론',\n",
       " '[10th REPLAY] 헤어질 결심',\n",
       " '어메이징 모리스',\n",
       " '새비지 맨',\n",
       " '[10th REPLAY] 에브리씽 에브리웨어 올 앳 원스',\n",
       " '캐리와 슈퍼콜라',\n",
       " '극장판 전생했더니 슬라임이었던 건에 대하여: 홍련의 인연',\n",
       " '[10th REPLAY] 사랑할 땐 누구나 최악이 된다',\n",
       " '[베로나 오페라 페스티벌] 투란도트',\n",
       " '애프터썬',\n",
       " '[10th REPLAY] 더 파더',\n",
       " '[10th REPLAY] 매스',\n",
       " '[10th REPLAY] 레 미제라블',\n",
       " '[10th REPLAY] 자마',\n",
       " '[10th REPLAY] 피닉스',\n",
       " '[10th REPLAY] 타오르는 여인의 초상',\n",
       " '다음 소희',\n",
       " '천룡팔부: 교봉전',\n",
       " '라일 라일 크로커다일',\n",
       " '[영국 로열 발레] 다이아몬드 기념 공연',\n",
       " '[ROH 발레] 달콤 쌉사름한 초콜릿',\n",
       " '라인',\n",
       " '이마 베프',\n",
       " '극장판 주술회전 0',\n",
       " '400번의 구타',\n",
       " '원피스 필름 레드',\n",
       " '애프터 양',\n",
       " '올빼미',\n",
       " '엄마의 땅: 그리샤와 숲의 주인',\n",
       " '프로메어',\n",
       " '극장판 파워레인저 캡틴포스: 지구를 위한 싸움',\n",
       " '유랑의 달',\n",
       " '쥴 앤 짐',\n",
       " '3000년의 기다림',\n",
       " '트윈',\n",
       " '해시태그 시그네',\n",
       " '양자경의 더 모든 날 모든 순간',\n",
       " '새를 사랑한 화가',\n",
       " '스위치',\n",
       " '가가린',\n",
       " '단순한 열정',\n",
       " '페르시아어 수업',\n",
       " '러브레터',\n",
       " '신비아파트 극장판 차원도깨비와 7개의 세계',\n",
       " '열여덟, 어른이 되는 나이',\n",
       " '시간을 꿈꾸는 소녀',\n",
       " '탄생',\n",
       " '헤어질 결심',\n",
       " '핑크퐁 시네마 콘서트 2: 원더스타 콘서트 대작전',\n",
       " '눈의 여왕5: 스노우 프린세스와 미러랜드의 비밀',\n",
       " '젠틀맨',\n",
       " '5월 이후',\n",
       " '데몬러버',\n",
       " '몬스터 하우스2: 인비져블 피닉스',\n",
       " '궁지에 몰린 쥐는 치즈 꿈을 꾼다',\n",
       " '네가 떨어뜨린 푸른 하늘',\n",
       " '디텍티브 나이트: 가면의 밤',\n",
       " '성스러운 거미',\n",
       " '안녕, 소중한 사람',\n",
       " '어쩌면 우린 헤어졌는지 모른다',\n",
       " '타이타닉: 25주년',\n",
       " '라스트 버스',\n",
       " '울프 하운드',\n",
       " '원 웨이',\n",
       " '두다다쿵: 후후섬의 비밀',\n",
       " '피터 본 칸트',\n",
       " '스톰보이',\n",
       " '[2023 사건 읽는 영화관]',\n",
       " '[사건 읽는 영화관] 대한민국 강력사건 : 연쇄살인사건',\n",
       " '[베로나 오페라 페스티벌 앙코르] 카르멘',\n",
       " '살수',\n",
       " '기적을 믿는 소녀',\n",
       " '서치 2',\n",
       " '앤트맨과 와스프: 퀀텀매니아',\n",
       " '카운트',\n",
       " '멍뭉이',\n",
       " '스즈메의 문단속',\n",
       " '그대 어이가리',\n",
       " '던전 앤 드래곤: 도적들의 명예',\n",
       " '에브리씽 에브리웨어 올 앳 원스',\n",
       " '콜 제인',\n",
       " '슈퍼 마리오 브라더스']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#영화제목\n",
    "#document.querySelector(\"#movieList > li:nth-child(1) > div.tit-area > p.tit\")\n",
    "\n",
    "#이건 한꺼풀 눌러들어갔을때. #document.querySelector(\"#contents > div.movie-detail-page > div.movie-detail-cont > p.title\")\n",
    "\n",
    "movies_name = soup.select('div.tit-area > p.tit')\n",
    "name_list = []\n",
    "\n",
    "for name in movies_name:\n",
    "    name_list.append(name.text)\n",
    "    \n",
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['예매율 23.7%',\n",
       " '예매율 19.5%',\n",
       " '예매율 8.9%',\n",
       " '예매율 5.1%',\n",
       " '예매율 4.2%',\n",
       " '예매율 3.9%',\n",
       " '예매율 3.8%',\n",
       " '예매율 3.4%',\n",
       " '예매율 2.4%',\n",
       " '예매율 2.4%',\n",
       " '예매율 2%',\n",
       " '예매율 1.8%',\n",
       " '예매율 1.7%',\n",
       " '예매율 1.4%',\n",
       " '예매율 1.3%',\n",
       " '예매율 1.3%',\n",
       " '예매율 1.2%',\n",
       " '예매율 0.9%',\n",
       " '예매율 0.9%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.5%',\n",
       " '예매율 0.4%',\n",
       " '예매율 0.2%',\n",
       " '예매율 0.2%',\n",
       " '예매율 0.2%',\n",
       " '예매율 0.2%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#영화 예매율\n",
    "#document.querySelector(\"#movieList > li:nth-child(1) > div.rate-date > span.rate\")\n",
    "\n",
    "# document.querySelector(\"#contents > div.movie-detail-page > div.movie-detail-cont > div.info > div.rate > p.cont\")\n",
    "booking_rate = soup.select('div.rate-date > span.rate')\n",
    "\n",
    "rate_list = []\n",
    "\n",
    "for rate in booking_rate:\n",
    "    rate_list.append(rate.text)\n",
    "    \n",
    "rate_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['개봉일 2023.01.04',\n",
       " '개봉일 2022.12.14',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2022.12.21',\n",
       " '개봉일 2023.01.04',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2022.11.30',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2023.02.19',\n",
       " '개봉일 2023.02.15',\n",
       " '개봉일 2023.01.26',\n",
       " '개봉일 2023.02.12',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2023.02.02',\n",
       " '개봉일 2023.02.05',\n",
       " '개봉일 2023.01.31',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2023.02.03',\n",
       " '개봉일 2023.02.11',\n",
       " '개봉일 2023.02.17',\n",
       " '개봉일 2023.02.18',\n",
       " '개봉일 2023.02.10',\n",
       " '개봉일 2023.02.04',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2023.01.16',\n",
       " '개봉일 2023.02.13',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2022.02.17',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2022.11.30',\n",
       " '개봉일 2022.06.01',\n",
       " '개봉일 2022.11.23',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2022.10.20',\n",
       " '개봉일 2023.01.19',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.04',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.01.11',\n",
       " '개봉일 2022.11.23',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.04',\n",
       " '개봉일 2022.12.22',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2022.12.15',\n",
       " '개봉일 2022.12.08',\n",
       " '개봉일 2022.12.14',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.11',\n",
       " '개봉일 2022.11.30',\n",
       " '개봉일 2022.06.29',\n",
       " '개봉일 2022.12.21',\n",
       " '개봉일 2022.12.22',\n",
       " '개봉일 2022.12.28',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2023.02.02',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.09',\n",
       " '개봉일 2023.02.09',\n",
       " '개봉일 2023.02.09',\n",
       " '개봉일 2023.02.15',\n",
       " '개봉일 2023.02.15',\n",
       " '개봉일 2023.02.16',\n",
       " '개봉일 2023.02.19',\n",
       " '개봉일 2023.02.19',\n",
       " '개봉일 2023.02.21',\n",
       " '개봉일 2023.02.22',\n",
       " '개봉일 2023.02',\n",
       " '개봉일 2023.02',\n",
       " '개봉일 2023.02',\n",
       " '개봉일 2023.02',\n",
       " '개봉일 2023.03.01',\n",
       " '개봉일 2023.03.08',\n",
       " '개봉일 2023.03',\n",
       " '개봉일 2023.03',\n",
       " '개봉일 2023.03',\n",
       " '개봉일 2023.03',\n",
       " '개봉일 2023.05.03']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#개봉일\n",
    "# document.querySelector(\"#movieList > li:nth-child(1) > div.rate-date > span.date\")\n",
    "\n",
    "# document.querySelector(\"#contentData > div:nth-child(5) > div.movie-info.infoContent > div > p:nth-child(4)\")\n",
    "date_open = soup.select('div.rate-date > span.date')\n",
    "date_list = []\n",
    "\n",
    "for open in date_open:\n",
    "    date_list.append(open.text)             \n",
    "            \n",
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화평점,장르,누적관객수는 한꺼풀 들어가야 나오니까\n",
    "\n",
    "html = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9.4']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#영화평점\n",
    "# \n",
    "\n",
    "# document.querySelector(\"#mainMegaScore > p > em\")\n",
    "movies_score = soup.select('#mainMegaScore > p > em')\n",
    "score_list = []\n",
    "\n",
    "for score in movies_score:\n",
    "    score_list.append(score.text)\n",
    "    \n",
    "score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['등급\\xa0: 등급미정']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#장르 \n",
    "# \n",
    "\n",
    "#document.querySelector(\"#contentData > div:nth-child(5) > div.movie-info.infoContent > div > p:nth-child(2)\")\n",
    "movies_genre = soup.select('div.movie-info.infoContent > div > p:nth-child(2)')\n",
    "genre_list = []\n",
    "\n",
    "for genre in movies_genre:\n",
    "    genre_list.append(genre.text)\n",
    "    \n",
    "genre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#누적관객수 (only mega, lotte)\n",
    "#document.querySelector(\"#contentData > div:nth-child(5) > div.movie-graph.infoContent > div:nth-child(3) > dl > dd\")\n",
    "#document.querySelector(\"#contents > div.movie-detail-page > div.movie-detail-cont > div.info > div.audience > p > em\")\n",
    "\n",
    "# 이게 상시바뀌는 데이터라 0으로 뜨는거같은데\n",
    "\n",
    "# document.querySelector(\"#contents > div.movie-detail-page > div.movie-detail-cont > div.info > div.audience > p > em\")\n",
    "# //*[@id=\"contents\"]/div[1]/div[4]/div[2]/div[3]/p/em\n",
    "\n",
    "movies_spect = soup.select('#contents > div.movie-detail-page > div.movie-detail-cont > div.info > div.audience > p > em')\n",
    "spect_list = []\n",
    "\n",
    "for spect in movies_spect:\n",
    "    spect_list.append(spect.text)\n",
    "    \n",
    "spect_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 81) :\n",
    "    \n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    \n",
    "    # 평점은 밖에서 뽑고\n",
    "#     score = soup.select('ul.img_list > li > dl > dd > div.rating_type > strong')[i-1].text\n",
    "    \n",
    "#     driver.find_element(By.XPATH,f'//*[@id=\"content\"]/div[4]/ul/li[{i}]/dl/dd[3]/a').click()\n",
    "#     time.sleep(1.5)\n",
    "    \n",
    "    # 클릭하고 들어가서 다시 로딩해주고 안에서 제목,작가,장르,연령대,좋아요,최근평점 까지 뽑는구나\n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "#     title = soup.select('h2 > span.title')[0].text\n",
    "#     author = soup.select('h2 > span.wrt_nm')[0].text\n",
    "#     genre = soup.select('span.genre')[0].text\n",
    "#     age = soup.select('span.age')[0].text\n",
    "#     like = soup.select('em.u_cnt')[0].text\n",
    "#     latest_score = soup.select('div.rating_type > strong')[0].text\n",
    "    \n",
    "#     driver.back()\n",
    "#     time.sleep(1.5)\n",
    "    \n",
    "    \n",
    "#     title_l.append(title)\n",
    "#     author_l.append(author)\n",
    "#     score_l.append(score)\n",
    "#     genre_l.append(genre)\n",
    "#     age_l.append(age)\n",
    "#     like_l.append(like)\n",
    "#     latest_l.append(latest_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "    \n",
    "for i in range(1, 91) :       \n",
    "    # 더보기는 다 잘 눌러주는데, 첫번째꺼만 들어갔다 나오고선 튕김.     \n",
    "    # 결국 시간의 여유가 더있었더라면 더보기클릭까지 구현했을거같은데 그냥..힘들어서 내가 더보기 다눌러주고 시작하는걸로했다.\n",
    "    # for j in range(4):\n",
    "    #     if j <= 4:\n",
    "    #         driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "    #         time.sleep(2)\n",
    "    #     elif j > 4:\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         pass        \n",
    "            \n",
    "    # name_list = []\n",
    "    # rate_list = []\n",
    "    # date_list = []\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # [0].text를 빼고 돌리면 다나오기는 하는데 정제가 안되서 나오고, 넣고 돌리면 슬램덩크 하나밖에없네 항목이\n",
    "    # 한꺼풀 들어가기전애들만 i로 바꾸고 돌려줬는데 이번엔 맨마지막꺼인 슈퍼마리오브라더스 만 나오네 미치곘네 7분계속버려\n",
    "    # i+1도 마찬가지 \n",
    "    # 그냥 다하고 정제를 조금 해주는식으로 진행. \n",
    "    # 이 가능할줄알았으나 안에서 긁어야되는 애들은 하나밖에안긁어지네 뭐지 도대체 \n",
    "    # 안긁히는 뒤에애들 for문으로 처리 진행. 아니 건들지도않은 앞에애들이 아예안긁히네 뒤에놈들은 그대로고 부셔버리고싶네 \n",
    "    \n",
    "    # 유레카.. 들어가는애들이랑 안들어가는애들이랑 나눠서 해주니까 잘긁힌다. 근데 왜 안들어가는애들 [0].text만 하면 \n",
    "    # 첫번째 애(슬램덩크)만 91개가 긁어져있지? \n",
    "    \n",
    "    # 드디어됐다. i-1 이 답이었네 왜냐하면 밖에있는애들은 이미 91개가 다있는상황이니까 한가지 숫자로만하면 한가지값만 출력이\n",
    "    # 되기때문에 i는 1부터시작이니 0부터 90까지 하나씩 다긁어주려면 이게 답이었네. 아ㅏ아아아\n",
    "                                       \n",
    "    \n",
    "    movies_name = soup.select('div.tit-area > p.tit')[i-1].text\n",
    "    booking_rate = soup.select('div.rate-date > span.rate')[i-1].text\n",
    "    date_open = soup.select('div.rate-date > span.date')[i-1].text\n",
    "    \n",
    "    # driver.find_element(By.XPATH,f'//*[@id=\"movieList\"]/li[{i}]/div[1]/div[3]/a').send_keys(Keys.ENTER)\n",
    "    # time.sleep(2)\n",
    "    \n",
    "    # html = driver.page_source\n",
    "    # soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # movies_score = soup.select('#mainMegaScore > p > em')\n",
    "    # # for score in movies_score:\n",
    "    # #     score_list.append(score.text)\n",
    "        \n",
    "    # movies_genre = soup.select('div.movie-info.infoContent > div > p:nth-child(2)')\n",
    "    # # for genre in movies_genre:\n",
    "    # #     genre_list.append(genre.text)\n",
    "        \n",
    "    # movies_spect = soup.select('div.movie-detail-page > div.movie-detail-cont > div.info > div.audience > p > em')\n",
    "    # # for spect in movies_spect:\n",
    "    # #     spect_list.append(spect.text)\n",
    "    \n",
    "    # driver.back()\n",
    "    # time.sleep(2)\n",
    "    \n",
    "    # for j in range(4):\n",
    "    #     if j <= 4:\n",
    "    #         driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "    #         time.sleep(2)\n",
    "    #     elif j > 4:\n",
    "    #         pass\n",
    "    #     else:\n",
    "    #         pass\n",
    "    \n",
    "    # if i == 20:\n",
    "    #     driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "    #     time.sleep(2)\n",
    "    # elif i == 40:\n",
    "    #     driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "    #     time.sleep(2)\n",
    "    # elif i == 60:\n",
    "    #     driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "    #     time.sleep(2)\n",
    "    # elif i == 80:\n",
    "    #     driver.find_element(By.XPATH,f'//*[@id=\"addMovieDiv\"]').click()\n",
    "    #     time.sleep(2)\n",
    "        \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "    \n",
    "    name_list.append(movies_name)\n",
    "    rate_list.append(booking_rate)\n",
    "    date_list.append(date_open)\n",
    "        \n",
    "    # 7분 6.4초 소요\n",
    "    # 31초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "genre_list = []\n",
    "spect_list = []\n",
    "\n",
    "for i in range(1, 91):\n",
    "\n",
    "    driver.find_element(By.XPATH,f'//*[@id=\"movieList\"]/li[{i}]/div[1]/div[3]/a').send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    movies_score = soup.select('#mainMegaScore > p > em')[0].text\n",
    "    # for score in movies_score:\n",
    "    #     score_list.append(score.text)\n",
    "        \n",
    "    movies_genre = soup.select('div.movie-info.infoContent > div > p:nth-child(2)')[0].text\n",
    "    # for genre in movies_genre:\n",
    "    #     genre_list.append(genre.text)\n",
    "        \n",
    "    movies_spect = soup.select('div.movie-detail-page > div.movie-detail-cont > div.info > div.audience > p > em')[0].text\n",
    "    # for spect in movies_spect:\n",
    "    #     spect_list.append(spect.text)\n",
    "    \n",
    "    driver.back()\n",
    "    time.sleep(2)\n",
    "\n",
    "    score_list.append(movies_score)\n",
    "    genre_list.append(movies_genre)\n",
    "    spect_list.append(movies_spect)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['더 퍼스트 슬램덩크',\n",
       " '아바타: 물의 길',\n",
       " '교섭',\n",
       " '상견니',\n",
       " '유령',\n",
       " '장화신은 고양이: 끝내주는 모험',\n",
       " '영웅',\n",
       " '메간',\n",
       " '바빌론',\n",
       " '오늘 밤, 세계에서 이 사랑이 사라진다 해도',\n",
       " '[10th REPLAY] 헤어질 결심',\n",
       " '어메이징 모리스',\n",
       " '새비지 맨',\n",
       " '극장판 전생했더니 슬라임이었던 건에 대하여: 홍련의 인연',\n",
       " '[10th REPLAY] 에브리씽 에브리웨어 올 앳 원스',\n",
       " '[10th REPLAY] 사랑할 땐 누구나 최악이 된다',\n",
       " '[베로나 오페라 페스티벌] 투란도트',\n",
       " '애프터썬',\n",
       " '캐리와 슈퍼콜라',\n",
       " '[10th REPLAY] 더 파더',\n",
       " '[10th REPLAY] 매스',\n",
       " '[10th REPLAY] 레 미제라블',\n",
       " '[10th REPLAY] 자마',\n",
       " '[10th REPLAY] 피닉스',\n",
       " '[10th REPLAY] 타오르는 여인의 초상',\n",
       " '다음 소희',\n",
       " '천룡팔부: 교봉전',\n",
       " '라일 라일 크로커다일',\n",
       " '[영국 로열 발레] 다이아몬드 기념 공연',\n",
       " '[ROH 발레] 달콤 쌉사름한 초콜릿',\n",
       " '라인',\n",
       " '이마 베프',\n",
       " '극장판 주술회전 0',\n",
       " '400번의 구타',\n",
       " '원피스 필름 레드',\n",
       " '애프터 양',\n",
       " '올빼미',\n",
       " '엄마의 땅: 그리샤와 숲의 주인',\n",
       " '프로메어',\n",
       " '극장판 파워레인저 캡틴포스: 지구를 위한 싸움',\n",
       " '유랑의 달',\n",
       " '쥴 앤 짐',\n",
       " '3000년의 기다림',\n",
       " '트윈',\n",
       " '해시태그 시그네',\n",
       " '양자경의 더 모든 날 모든 순간',\n",
       " '새를 사랑한 화가',\n",
       " '스위치',\n",
       " '가가린',\n",
       " '단순한 열정',\n",
       " '페르시아어 수업',\n",
       " '러브레터',\n",
       " '신비아파트 극장판 차원도깨비와 7개의 세계',\n",
       " '열여덟, 어른이 되는 나이',\n",
       " '시간을 꿈꾸는 소녀',\n",
       " '탄생',\n",
       " '헤어질 결심',\n",
       " '핑크퐁 시네마 콘서트 2: 원더스타 콘서트 대작전',\n",
       " '눈의 여왕5: 스노우 프린세스와 미러랜드의 비밀',\n",
       " '젠틀맨',\n",
       " '5월 이후',\n",
       " '데몬러버',\n",
       " '몬스터 하우스2: 인비져블 피닉스',\n",
       " '궁지에 몰린 쥐는 치즈 꿈을 꾼다',\n",
       " '네가 떨어뜨린 푸른 하늘',\n",
       " '디텍티브 나이트: 가면의 밤',\n",
       " '성스러운 거미',\n",
       " '안녕, 소중한 사람',\n",
       " '어쩌면 우린 헤어졌는지 모른다',\n",
       " '타이타닉: 25주년',\n",
       " '라스트 버스',\n",
       " '울프 하운드',\n",
       " '원 웨이',\n",
       " '두다다쿵: 후후섬의 비밀',\n",
       " '피터 본 칸트',\n",
       " '스톰보이',\n",
       " '[2023 사건 읽는 영화관]',\n",
       " '[사건 읽는 영화관] 대한민국 강력사건 : 연쇄살인사건',\n",
       " '[베로나 오페라 페스티벌 앙코르] 카르멘',\n",
       " '살수',\n",
       " '기적을 믿는 소녀',\n",
       " '서치 2',\n",
       " '앤트맨과 와스프: 퀀텀매니아',\n",
       " '카운트',\n",
       " '멍뭉이',\n",
       " '스즈메의 문단속',\n",
       " '그대 어이가리',\n",
       " '던전 앤 드래곤: 도적들의 명예',\n",
       " '에브리씽 에브리웨어 올 앳 원스',\n",
       " '콜 제인']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['예매율 22.9%',\n",
       " '예매율 17.8%',\n",
       " '예매율 9.9%',\n",
       " '예매율 5.1%',\n",
       " '예매율 4.6%',\n",
       " '예매율 3.9%',\n",
       " '예매율 3.9%',\n",
       " '예매율 3.7%',\n",
       " '예매율 2.8%',\n",
       " '예매율 2.5%',\n",
       " '예매율 2.2%',\n",
       " '예매율 2%',\n",
       " '예매율 1.6%',\n",
       " '예매율 1.5%',\n",
       " '예매율 1.5%',\n",
       " '예매율 1.3%',\n",
       " '예매율 1.1%',\n",
       " '예매율 1%',\n",
       " '예매율 1%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.8%',\n",
       " '예매율 0.5%',\n",
       " '예매율 0.4%',\n",
       " '예매율 0.2%',\n",
       " '예매율 0.2%',\n",
       " '예매율 0.2%',\n",
       " '예매율 0.2%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0.1%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%',\n",
       " '예매율 0%']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['개봉일 2023.01.04',\n",
       " '개봉일 2022.12.14',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2023.01.04',\n",
       " '개봉일 2022.12.21',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2022.11.30',\n",
       " '개봉일 2023.02.19',\n",
       " '개봉일 2023.02.15',\n",
       " '개봉일 2023.01.26',\n",
       " '개봉일 2023.02.02',\n",
       " '개봉일 2023.02.12',\n",
       " '개봉일 2023.02.05',\n",
       " '개봉일 2023.01.31',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2023.02.03',\n",
       " '개봉일 2023.02.11',\n",
       " '개봉일 2023.02.17',\n",
       " '개봉일 2023.02.18',\n",
       " '개봉일 2023.02.10',\n",
       " '개봉일 2023.02.04',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2023.01.16',\n",
       " '개봉일 2023.02.13',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2022.02.17',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2022.11.30',\n",
       " '개봉일 2022.06.01',\n",
       " '개봉일 2022.11.23',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2022.10.20',\n",
       " '개봉일 2023.01.19',\n",
       " '개봉일 2023.01.18',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.04',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.01.11',\n",
       " '개봉일 2022.11.23',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.04',\n",
       " '개봉일 2022.12.22',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2022.12.15',\n",
       " '개봉일 2022.12.08',\n",
       " '개봉일 2022.12.14',\n",
       " '개봉일 2023.01.25',\n",
       " '개봉일 2023.01.11',\n",
       " '개봉일 2022.11.30',\n",
       " '개봉일 2022.06.29',\n",
       " '개봉일 2022.12.21',\n",
       " '개봉일 2022.12.22',\n",
       " '개봉일 2022.12.28',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2023.02.01',\n",
       " '개봉일 2023.02.02',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.08',\n",
       " '개봉일 2023.02.09',\n",
       " '개봉일 2023.02.09',\n",
       " '개봉일 2023.02.09',\n",
       " '개봉일 2023.02.15',\n",
       " '개봉일 2023.02.15',\n",
       " '개봉일 2023.02.16',\n",
       " '개봉일 2023.02.19',\n",
       " '개봉일 2023.02.19',\n",
       " '개봉일 2023.02.21',\n",
       " '개봉일 2023.02.22',\n",
       " '개봉일 2023.02',\n",
       " '개봉일 2023.02',\n",
       " '개봉일 2023.02',\n",
       " '개봉일 2023.02',\n",
       " '개봉일 2023.03.01',\n",
       " '개봉일 2023.03.08',\n",
       " '개봉일 2023.03',\n",
       " '개봉일 2023.03',\n",
       " '개봉일 2023.03',\n",
       " '개봉일 2023.03']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "근데 얘네는 왜 이렇게 여기선 한가지값만 뜰까 // 저ㅏ장하면 다 잘떠있던데 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'장르\\xa0: 드라마 / 122 분'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([name_list,rate_list,date_list,score_list,genre_list,spect_list])\n",
    "mega = data.T\n",
    "mega.columns = ['영화제목', '예매율', '개봉일', '평점','장르','누적관객수']\n",
    "mega.to_excel('megaboxTEST2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로띠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로띠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로띠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로띠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ks2\\AppData\\Local\\Temp\\ipykernel_12836\\2440289425.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('chromedriver.exe')\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "url = 'https://www.lottecinema.co.kr/NLCHS/Movie/List?flag=1'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 더보기 후 다시로딩 \n",
    "# 더보기까지 누르는거 구현하는거는 일단 메가박스에서 시달리면서 내려놓게 되었다. \n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['더 퍼스트 슬램덩크']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영화제목 (현재상영작)\n",
    "\n",
    "# document.querySelector(\"#contents > div > div.tit_info\")\n",
    "# //*[@id=\"contents\"]/div/div[2]/strong\n",
    "\n",
    "movies_name = soup.select('#contents > div > div.tit_info > strong')\n",
    "name_list = []\n",
    "\n",
    "for name in movies_name:\n",
    "    name_list.append(name.text)\n",
    "    \n",
    "name_list\n",
    "\n",
    "\n",
    "# 영화제목 (상영예정작_아직 개봉 한참 남은)\n",
    "\n",
    "# 3번째 _ 애프터썬\n",
    "# document.querySelector(\"#contents > div > div.tit_info > strong\")\n",
    "# //*[@id=\"contents\"]/div/div[2]/strong\n",
    "\n",
    "# 7번째?_안녕 소중한사람\n",
    "# document.querySelector(\"#contents > div > div.tit_info > strong\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['27.0% ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예매율 (현재상영작)\n",
    "\n",
    "# document.querySelector(\"#contents > div > ul.detail_info1 > li.sub_info2 > strong\")\n",
    "# //*[@id=\"contents\"]/div/ul[1]/li[2]/strong\n",
    "\n",
    "booking_rate = soup.select('div > ul.detail_info1 > li.sub_info2 > strong')\n",
    "\n",
    "rate_list = []\n",
    "\n",
    "for rate in booking_rate:\n",
    "    rate_list.append(rate.text)\n",
    "    \n",
    "rate_list   \n",
    "\n",
    "# document.querySelector(\"#contents > div > ul.detail_info1 > li.sub_info2 > strong\")\n",
    "# //*[@id=\"contents\"]/div/ul[1]/li[2]/strong\n",
    "\n",
    "# document.querySelector(\"#contents > div > ul.detail_info1 > li.sub_info2 > strong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023.01.04 개봉']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 개봉일 (현재상영작)\n",
    "\n",
    "# document.querySelector(\"#contents > div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)\")\n",
    "# //*[@id=\"contents\"]/div/ul[2]/li[1]/strong/em[2]\n",
    "\n",
    "date_open = soup.select('div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)')\n",
    "date_list = []\n",
    "\n",
    "for open in date_open:\n",
    "    date_list.append(open.text)             \n",
    "            \n",
    "date_list\n",
    "\n",
    "\n",
    "# document.querySelector(\"#contents > div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9.7 ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 평점 (현재상영작)\n",
    "\n",
    "# document.querySelector(\"#contents > div > ul.detail_info1 > li.sub_info1 > strong > strong\")\n",
    "# //*[@id=\"contents\"]/div/ul[1]/li[1]/strong/strong\n",
    "\n",
    "movies_score = soup.select('div > ul.detail_info1 > li.sub_info1 > strong > strong')\n",
    "score_list = []\n",
    "\n",
    "for score in movies_score:\n",
    "    score_list.append(score.text)\n",
    "    \n",
    "score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['애니메이션 / 일본']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 장르 (현재상영작)\n",
    "\n",
    "# document.querySelector(\"#contents > div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)\")\n",
    "# //*[@id=\"contents\"]/div/ul[2]/li[1]/strong/em[1]\n",
    "\n",
    "movies_genre = soup.select('ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)')\n",
    "genre_list = []\n",
    "\n",
    "for genre in movies_genre:\n",
    "    genre_list.append(genre.text)\n",
    "    \n",
    "genre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,824,459 명  ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 관객수 (현재상영작 / only lotte, megabox)\n",
    "\n",
    "# document.querySelector(\"#contents > div > ul.detail_info1 > li.sub_info3 > strong\")\n",
    "# //*[@id=\"contents\"]/div/ul[1]/li[3]/strong\n",
    "\n",
    "movies_spect = soup.select('div > ul.detail_info1 > li.sub_info3 > strong')\n",
    "spect_list = []\n",
    "\n",
    "for spect in movies_spect:\n",
    "    spect_list.append(spect.text)\n",
    "    \n",
    "spect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 __ \n",
    "for i in range(1, 28) :\n",
    "    try : \n",
    "        sample = driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div/ul[4]/li[{i}]/div[1]/div/div/a[2]')\n",
    "        driver.execute_script('arguments[0].click();', sample)\n",
    "        time.sleep(2)\n",
    "        driver.back()\n",
    "        time.sleep(2)\n",
    "    \n",
    "    except :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ks2\\AppData\\Local\\Temp\\ipykernel_12836\\2440289425.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('chromedriver.exe')\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "url = 'https://www.lottecinema.co.kr/NLCHS/Movie/List?flag=1'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 롯데시네마는 다 한꺼풀 들어가서 긁기로 했으니까 \n",
    "# 상세보기 : \n",
    "# 1영화 \n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(1) > div.top_info > div > div > a:nth-child(2)\")\n",
    "# //*[@id=\"contents\"]/div/ul[4]/li[1]/div[1]/div/div/a[2]\n",
    "# 2영화 \n",
    "# //*[@id=\"contents\"]/div/ul[4]/li[2]/div[1]/div/div/a[2]\n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(2) > div.top_info > div > div > a:nth-child(2)\")\n",
    "# 마지막(32번째) 영화\n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(32) > div.top_info > div > div > a:nth-child(2)\")\n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(32) > div.top_info > div > div > a:nth-child(2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재상영작 \n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "score_list = []\n",
    "genre_list = []\n",
    "spect_list = []\n",
    "\n",
    "for i in range(1, 33):\n",
    "    try : \n",
    "        sample = driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div/ul[4]/li[{i}]/div[1]/div/div/a[2]')\n",
    "        driver.execute_script('arguments[0].click();', sample)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        movies_name = soup.select('#contents > div > div.tit_info > strong')[0].text\n",
    "        booking_rate = soup.select('div > ul.detail_info1 > li.sub_info2 > strong')[0].text\n",
    "        date_open = soup.select('div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)')[0].text\n",
    "        movies_score = soup.select('div > ul.detail_info1 > li.sub_info1 > strong > strong')[0].text\n",
    "        movies_genre = soup.select('ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)')[0].text\n",
    "        movies_spect = soup.select('div > ul.detail_info1 > li.sub_info3 > strong')[0].text\n",
    "\n",
    "        \n",
    "        driver.back()\n",
    "        time.sleep(2)\n",
    "    \n",
    "    except :\n",
    "        pass\n",
    "    \n",
    "    name_list.append(movies_name)\n",
    "    rate_list.append(booking_rate)\n",
    "    date_list.append(date_open)\n",
    "    score_list.append(movies_score)\n",
    "    genre_list.append(movies_genre)\n",
    "    spect_list.append(movies_spect)   \n",
    "    \n",
    "# 아바타(3번째)에서 멈추네\n",
    "# try except넣어주니까 잘되네       근데 갑자기 range 헷갈리네 마지막꺼 포함안하는거 맞지?? 맞네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([name_list,rate_list,date_list,score_list,genre_list,spect_list])\n",
    "lotte = data.T\n",
    "lotte.columns = ['영화제목', '예매율', '개봉일', '평점','장르','누적관객수']\n",
    "lotte.to_excel('lotteTEST.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상영 예정작\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lotte 상영예정작 path 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아 이게 path들을 보면 개봉이 얼마 안남은 애들은 상세보기위에 예매하기까지 클릭버튼이 생겨있고 아닌애들은 상세보기만 있네\n",
    "# 그래서 둘이 path가 조금 다르네\n",
    "# 처음긁으니까 개봉얼마안남은 바빌론 이마베프 몬스터하우스2 로 34개가 다 채워졌다.  \n",
    "\n",
    "# 첫번째 영화(개봉근접 현시점 3개 - 1,2,5번째 1,2,6)\n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(1) > div.top_info > div > div > a:nth-child(2)\")\n",
    "# //*[@id=\"contents\"]/div/ul[4]/li[1]/div[1]/div/div/a[2]\n",
    "# 두번째\n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(2) > div.top_info > div > div > a:nth-child(2)\")\n",
    "# //*[@id=\"contents\"]/div/ul[4]/li[2]/div[1]/div/div/a[2]\n",
    "\n",
    "# 4번째\n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(4) > div.top_info > div > div > a\")\n",
    "# //*[@id=\"contents\"]/div/ul[4]/li[4]/div[1]/div/div/a\n",
    "\n",
    "# 광고\n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(5) > a > img\")\n",
    "# //*[@id=\"contents\"]/div/ul[4]/li[5]/a/img\n",
    "# 아니 롯데 얘네 광고도 리스트에 포함시키는건가 왜 5번쨰가 6이지\n",
    "\n",
    "# 5번째\n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(6) > div.top_info > div > div > a:nth-child(2)\")\n",
    "# //*[@id=\"contents\"]/div/ul[4]/li[6]/div[1]/div/div/a[2]\n",
    "\n",
    "# 맨마지막 영화(그렇지않은 애들 31개)\n",
    "# document.querySelector(\"#contents > div > ul.movie_list.type2 > li:nth-child(34) > div.top_info > div > div > a\")\n",
    "# //*[@id=\"contents\"]/div/ul[4]/li[34]/div[1]/div/div/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(20):\n",
    "#     z = ['li:nth-child(1)','li:nth-child(2)','li:nth-child(3)', 'li:nth-child(4)', 'li:nth-child(5)', 'li:nth-child(6)','li:nth-child(7)','li:nth-child(8)','li:nth-child(9)', 'li:nth-child(10)','li:nth-child(11)','li:nth-child(12)','li:nth-child(13)','li:nth-child(14)','li:nth-child(15)','li:nth-child(16)','li:nth-child(17)','li:nth-child(18)','li:nth-child(19)','li:nth-child(20)'][i]\n",
    "#     data_list = soup.select(f'#movieList > {z} > div.movie-list-info > img')\n",
    "    \n",
    "#     #for j in range (len(data_list)):\n",
    "#     driver.find_element(By.XPATH, f'//*[@id=\"movieList\"]/li[{i+1}]/div[1]/div[3]/a').send_keys(Keys.ENTER)\n",
    "#     time.sleep(2)\n",
    "#     driver.back()\n",
    "#     time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 상영예정작_개봉근접. \n",
    "\n",
    "# 분리해서 뽑아보기 시행착오 \n",
    "\n",
    "\n",
    "# close_list = []\n",
    "# notyet_list = []\n",
    "# name_list = []\n",
    "# rate_list = []\n",
    "# date_list = []\n",
    "# score_list = []\n",
    "# genre_list = []\n",
    "# spect_list = []\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(3):\n",
    "#     z = ['li[1]','li[2]','li[6]']\n",
    "  \n",
    "#     sample = driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div/ul[4]/{z}/div[1]/div/div/a[2]')\n",
    "#     driver.execute_script('arguments[0].click();', sample)\n",
    "#     time.sleep(2)\n",
    "    \n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "#     movies_name = soup.select('#contents > div > div.tit_info > strong')[0].text\n",
    "#     booking_rate = soup.select('div > ul.detail_info1 > li.sub_info2 > strong')[0].text\n",
    "#     date_open = soup.select('div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)')[0].text\n",
    "#     movies_score = soup.select('div > ul.detail_info1 > li.sub_info1 > strong > strong')[0].text\n",
    "#     movies_genre = soup.select('ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)')[0].text\n",
    "#     movies_spect = soup.select('div > ul.detail_info1 > li.sub_info3 > strong')[0].text\n",
    "\n",
    "#     driver.back()\n",
    "#     time.sleep(2)\n",
    "        \n",
    "\n",
    "    \n",
    "#     name_list.append(movies_name)\n",
    "#     rate_list.append(booking_rate)\n",
    "#     date_list.append(date_open)\n",
    "#     score_list.append(movies_score)\n",
    "#     genre_list.append(movies_genre)\n",
    "#     spect_list.append(movies_spect)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상영예정작_개봉근접.  1번애만 뽑기\n",
    "\n",
    "close_list = []\n",
    "notyet_list = []\n",
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "score_list = []\n",
    "genre_list = []\n",
    "spect_list = []\n",
    "\n",
    "try : \n",
    "    sample = driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div/ul[4]/li[1]/div[1]/div/div/a[2]')\n",
    "    driver.execute_script('arguments[0].click();', sample)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    movies_name = soup.select('#contents > div > div.tit_info > strong')[0].text\n",
    "    booking_rate = soup.select('div > ul.detail_info1 > li.sub_info2 > strong')[0].text\n",
    "    date_open = soup.select('div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)')[0].text\n",
    "    movies_score = soup.select('div > ul.detail_info1 > li.sub_info1 > strong > strong')[0].text\n",
    "    movies_genre = soup.select('ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)')[0].text\n",
    "    movies_spect = soup.select('div > ul.detail_info1 > li.sub_info3 > strong')[0].text\n",
    "\n",
    "    \n",
    "    driver.back()\n",
    "    time.sleep(2)\n",
    "\n",
    "except :\n",
    "    pass\n",
    "\n",
    "name_list.append(movies_name)\n",
    "rate_list.append(booking_rate)\n",
    "date_list.append(date_open)\n",
    "score_list.append(movies_score)\n",
    "genre_list.append(movies_genre)\n",
    "spect_list.append(movies_spect)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상영예정작_개봉근접.  2번애만 뽑기\n",
    "\n",
    "close_list = []\n",
    "notyet_list = []\n",
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "score_list = []\n",
    "genre_list = []\n",
    "spect_list = []\n",
    "\n",
    "try : \n",
    "    sample = driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div/ul[4]/li[2]/div[1]/div/div/a[2]')\n",
    "    driver.execute_script('arguments[0].click();', sample)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    movies_name = soup.select('#contents > div > div.tit_info > strong')[0].text\n",
    "    booking_rate = soup.select('div > ul.detail_info1 > li.sub_info2 > strong')[0].text\n",
    "    date_open = soup.select('div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)')[0].text\n",
    "    movies_score = soup.select('div > ul.detail_info1 > li.sub_info1 > strong > strong')[0].text\n",
    "    movies_genre = soup.select('ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)')[0].text\n",
    "    movies_spect = soup.select('div > ul.detail_info1 > li.sub_info3 > strong')[0].text\n",
    "\n",
    "    \n",
    "    driver.back()\n",
    "    time.sleep(2)\n",
    "\n",
    "except :\n",
    "    pass\n",
    "\n",
    "name_list.append(movies_name)\n",
    "rate_list.append(booking_rate)\n",
    "date_list.append(date_open)\n",
    "score_list.append(movies_score)\n",
    "genre_list.append(movies_genre)\n",
    "spect_list.append(movies_spect)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상영예정작_개봉근접.  6번애만 뽑기\n",
    "\n",
    "close_list = []\n",
    "notyet_list = []\n",
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "score_list = []\n",
    "genre_list = []\n",
    "spect_list = []\n",
    "\n",
    "try : \n",
    "    sample = driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div/ul[4]/li[6]/div[1]/div/div/a[2]')\n",
    "    driver.execute_script('arguments[0].click();', sample)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    movies_name = soup.select('#contents > div > div.tit_info > strong')[0].text\n",
    "    booking_rate = soup.select('div > ul.detail_info1 > li.sub_info2 > strong')[0].text\n",
    "    date_open = soup.select('div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)')[0].text\n",
    "    movies_score = soup.select('div > ul.detail_info1 > li.sub_info1 > strong > strong')[0].text\n",
    "    movies_genre = soup.select('ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)')[0].text\n",
    "    movies_spect = soup.select('div > ul.detail_info1 > li.sub_info3 > strong')[0].text\n",
    "\n",
    "    \n",
    "    driver.back()\n",
    "    time.sleep(2)\n",
    "\n",
    "except :\n",
    "    pass\n",
    "\n",
    "name_list.append(movies_name)\n",
    "rate_list.append(booking_rate)\n",
    "date_list.append(date_open)\n",
    "score_list.append(movies_score)\n",
    "genre_list.append(movies_genre)\n",
    "spect_list.append(movies_spect)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([name_list,rate_list,date_list,score_list,genre_list,spect_list])\n",
    "lotte = data.T\n",
    "lotte.columns = ['영화제목', '예매율', '개봉일', '평점','장르','누적관객수']\n",
    "lotte.to_excel('lotteTEST_6번영화.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상영예정작_개봉아직멀은 애들 \n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상영예정작_개봉아직멀은 애들 \n",
    "# 아니도대체 왜 갑자기 예매 페이지로 넘어가는거냐고 \n",
    "# 혹시나 그 제목path나 뭐 예매율path가 기존 현재상영작 path랑 다른가 싶어서 비교해봤는데 다 똑같다. 뭔차이지 \n",
    "# 뭐때문에 계속 예매페이지로 넘어가지는거냐 진짜 \n",
    "\n",
    "close_list = []\n",
    "notyet_list = []\n",
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "score_list = []\n",
    "genre_list = []\n",
    "spect_list = []\n",
    "\n",
    "for i in range(3, 35):\n",
    "    try: \n",
    "        sample = driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div/ul[4]/li[{i}]/div[1]/div/div/a')\n",
    "        driver.execute_script('arguments[0].click();', sample)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        movies_name = soup.select('#contents > div > div.tit_info > strong')[0].text\n",
    "        booking_rate = soup.select('div > ul.detail_info1 > li.sub_info2 > strong')[0].text\n",
    "        date_open = soup.select('div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)')[0].text\n",
    "        movies_score = soup.select('div > ul.detail_info1 > li.sub_info1 > strong > strong')[0].text\n",
    "        movies_genre = soup.select('ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)')[0].text\n",
    "        movies_spect = soup.select('div > ul.detail_info1 > li.sub_info3 > strong')[0].text\n",
    "\n",
    "        \n",
    "        driver.back()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        pass   \n",
    "      \n",
    "        \n",
    "        name_list.append(movies_name)\n",
    "        rate_list.append(booking_rate)\n",
    "        date_list.append(date_open)\n",
    "        score_list.append(movies_score)\n",
    "        genre_list.append(movies_genre)\n",
    "        spect_list.append(movies_spect)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([name_list,rate_list,date_list,score_list,genre_list,spect_list])\n",
    "lotte = data.T\n",
    "lotte.columns = ['영화제목', '예매율', '개봉일', '평점','장르','누적관객수']\n",
    "lotte.to_excel('lotteTEST_notyet.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_list = []\n",
    "# rate_list = []\n",
    "# date_list = []\n",
    "# score_list = []\n",
    "# genre_list = []\n",
    "# spect_list = []\n",
    "\n",
    "# for i in range(1, 32):\n",
    "\n",
    "#     # 메가박스 했던거로는 아예 시작조차도 안되네 뭐지 \n",
    "#     # 분명히 눌러서 들어가는 구조는 똑같은데 음 \n",
    "#     # 엔터랑 클릭이랑 다를까 싶어서 바꿔서도 해봤는데 입밴이 여지없이 바로 튀어나오네. 아예 접근조차가 안된다는건데 \n",
    "#     # 그렇다고 jspath랑 xpath가 다르거나 틀리지도 않은거같은데 왜지 ,? \n",
    "#     driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div/ul[4]/li[{i}]/div[1]/div/div/a[2]').send_keys(Keys.ENTER)\n",
    "#     # driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div/ul[4]/li[1]/div[1]/div/div/a[2]').click()\n",
    "#     time.sleep(2)\n",
    "        \n",
    "#     html = driver.page_source\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#     movies_name = soup.select('#contents > div > div.tit_info > strong')[0].text\n",
    "#     booking_rate = soup.select('div > ul.detail_info1 > li.sub_info2 > strong')[0].text\n",
    "#     date_open = soup.select('div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)')[0].text\n",
    "#     movies_score = soup.select('div > ul.detail_info1 > li.sub_info1 > strong > strong')[0].text\n",
    "#     movies_genre = soup.select('ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)')[0].text\n",
    "#     movies_spect = soup.select('div > ul.detail_info1 > li.sub_info3 > strong')[0].text\n",
    "\n",
    "#     driver.back()\n",
    "#     time.sleep(2)\n",
    "\n",
    "#     name_list.append(movies_name)\n",
    "#     rate_list.append(booking_rate)\n",
    "#     date_list.append(date_open)\n",
    "#     score_list.append(movies_score)\n",
    "#     genre_list.append(movies_genre)\n",
    "#     spect_list.append(movies_spect)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메가박스 한꺼풀 들어가고 난후 긁는 코드\n",
    "score_list = []\n",
    "genre_list = []\n",
    "spect_list = []\n",
    "\n",
    "for i in range(1, 91):\n",
    "\n",
    "    driver.find_element(By.XPATH,f'//*[@id=\"movieList\"]/li[{i}]/div[1]/div[3]/a').send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    movies_score = soup.select('#mainMegaScore > p > em')[0].text\n",
    "    # for score in movies_score:\n",
    "    #     score_list.append(score.text)\n",
    "        \n",
    "    movies_genre = soup.select('div.movie-info.infoContent > div > p:nth-child(2)')[0].text\n",
    "    # for genre in movies_genre:\n",
    "    #     genre_list.append(genre.text)\n",
    "        \n",
    "    movies_spect = soup.select('div.movie-detail-page > div.movie-detail-cont > div.info > div.audience > p > em')[0].text\n",
    "    # for spect in movies_spect:\n",
    "    #     spect_list.append(spect.text)\n",
    "    \n",
    "    driver.back()\n",
    "    time.sleep(2)\n",
    "\n",
    "    score_list.append(movies_score)\n",
    "    genre_list.append(movies_genre)\n",
    "    spect_list.append(movies_spect)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"movieList\"]/li[3]/div[1]/div[3]/a\"}\n  (Session info: chrome=109.0.5414.120)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x00B86643]\n\t(No symbol) [0x00B1BE21]\n\t(No symbol) [0x00A1DA9D]\n\t(No symbol) [0x00A51342]\n\t(No symbol) [0x00A5147B]\n\t(No symbol) [0x00A88DC2]\n\t(No symbol) [0x00A6FDC4]\n\t(No symbol) [0x00A86B09]\n\t(No symbol) [0x00A6FB76]\n\t(No symbol) [0x00A449C1]\n\t(No symbol) [0x00A45E5D]\n\tGetHandleVerifier [0x00DFA142+2497106]\n\tGetHandleVerifier [0x00E285D3+2686691]\n\tGetHandleVerifier [0x00E2BB9C+2700460]\n\tGetHandleVerifier [0x00C33B10+635936]\n\t(No symbol) [0x00B24A1F]\n\t(No symbol) [0x00B2A418]\n\t(No symbol) [0x00B2A505]\n\t(No symbol) [0x00B3508B]\n\tBaseThreadInitThunk [0x75E67D69+25]\n\tRtlInitializeExceptionChain [0x76EEBB9B+107]\n\tRtlClearBits [0x76EEBB1F+191]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m spect_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m, \u001b[39m35\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m     driver\u001b[39m.\u001b[39;49mfind_element(By\u001b[39m.\u001b[39;49mXPATH,\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m//*[@id=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmovieList\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m]/li[\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m]/div[1]/div[3]/a\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39msend_keys(Keys\u001b[39m.\u001b[39mENTER)\n\u001b[0;32m     17\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m2\u001b[39m)\n\u001b[0;32m     19\u001b[0m     html \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mpage_source\n",
      "File \u001b[1;32mc:\\Y\\python 3.9.13\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:830\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    827\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    828\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 830\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\u001b[39m\"\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m\"\u001b[39;49m: by, \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m: value})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Y\\python 3.9.13\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    438\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    441\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Y\\python 3.9.13\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"movieList\"]/li[3]/div[1]/div[3]/a\"}\n  (Session info: chrome=109.0.5414.120)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x00B86643]\n\t(No symbol) [0x00B1BE21]\n\t(No symbol) [0x00A1DA9D]\n\t(No symbol) [0x00A51342]\n\t(No symbol) [0x00A5147B]\n\t(No symbol) [0x00A88DC2]\n\t(No symbol) [0x00A6FDC4]\n\t(No symbol) [0x00A86B09]\n\t(No symbol) [0x00A6FB76]\n\t(No symbol) [0x00A449C1]\n\t(No symbol) [0x00A45E5D]\n\tGetHandleVerifier [0x00DFA142+2497106]\n\tGetHandleVerifier [0x00E285D3+2686691]\n\tGetHandleVerifier [0x00E2BB9C+2700460]\n\tGetHandleVerifier [0x00C33B10+635936]\n\t(No symbol) [0x00B24A1F]\n\t(No symbol) [0x00B2A418]\n\t(No symbol) [0x00B2A505]\n\t(No symbol) [0x00B3508B]\n\tBaseThreadInitThunk [0x75E67D69+25]\n\tRtlInitializeExceptionChain [0x76EEBB9B+107]\n\tRtlClearBits [0x76EEBB1F+191]\n"
     ]
    }
   ],
   "source": [
    "# 혹시 메가박스 코드로 되나싶어서 해봤는데 바로 입밴먹는거보니까 try except써야되는건 맞아. \n",
    "\n",
    "\n",
    "# 상영예정작_개봉아직멀은 애들 \n",
    "# 아니도대체 왜 갑자기 예매 페이지로 넘어가는거냐고 \n",
    "# 혹시나 그 제목path나 뭐 예매율path가 기존 현재상영작 path랑 다른가 싶어서 비교해봤는데 다 똑같다. 뭔차이지 \n",
    "# 뭐때문에 계속 예매페이지로 넘어가지는거냐 진짜 \n",
    "\n",
    "# close_list = []\n",
    "# notyet_list = []\n",
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "score_list = []\n",
    "genre_list = []\n",
    "spect_list = []\n",
    "\n",
    "for i in range(3, 35):\n",
    "    driver.find_element(By.XPATH,f'//*[@id=\"movieList\"]/li[{i}]/div[1]/div[3]/a').send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    movies_name = soup.select('#contents > div > div.tit_info > strong')[0].text\n",
    "    booking_rate = soup.select('div > ul.detail_info1 > li.sub_info2 > strong')[0].text\n",
    "    date_open = soup.select('div > ul.detail_info2 > li.sub_info1 > strong > em:nth-child(2)')[0].text\n",
    "    movies_score = soup.select('div > ul.detail_info1 > li.sub_info1 > strong > strong')[0].text\n",
    "    movies_genre = soup.select('ul.detail_info2 > li.sub_info1 > strong > em:nth-child(1)')[0].text\n",
    "    movies_spect = soup.select('div > ul.detail_info1 > li.sub_info3 > strong')[0].text\n",
    "\n",
    "    \n",
    "    driver.back()\n",
    "    time.sleep(2)\n",
    "\n",
    "    \n",
    "    name_list.append(movies_name)\n",
    "    rate_list.append(booking_rate)\n",
    "    date_list.append(date_open)\n",
    "    score_list.append(movies_score)\n",
    "    genre_list.append(movies_genre)\n",
    "    spect_list.append(movies_spect)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cgv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cgv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cgv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ks2\\AppData\\Local\\Temp\\ipykernel_12836\\3784374292.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome('chromedriver.exe')\n"
     ]
    }
   ],
   "source": [
    "# 크롬 드라이버 실행\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "# url 접속하기\n",
    "url = 'http://www.cgv.co.kr/movies/?lt=1&ft=0'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path 51개여야되는데..? \n",
    "# 아 3덩이로 되어있네 \n",
    "# 1~7(1~3, 4~7) / 1~12(8~19) / 1~32(20~51) \n",
    "# document.querySelector(\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(2)\") \n",
    "#               / document.querySelector(\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(3)\")\n",
    "# document.querySelector(\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol.list-more\")\n",
    "# document.querySelector(\"#contents > div.sect-movie-chart\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 19 document.querySelector(\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol.list-more > li:nth-child(12) > div.box-contents > a > strong\")\n",
    "\n",
    "# 제목\n",
    "# document.querySelector(\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(2) > li:th-child(1) > div.box-contents > a > strong\")\n",
    "#안 document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.title > strong\")\n",
    "# //*[@id=\"contents\"]/div[1]/div[3]/ol[1]/li[1]/div[2]/a/strong\n",
    "\n",
    "# 예매율\n",
    "#밖 document.querySelector(\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(2) > li:nth-child(1) > div.box-contents > div > strong > span\")\n",
    "#안 document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.score > strong > span\")\n",
    "# //*[@id=\"contents\"]/div[1]/div[3]/ol[1]/li[1]/div[2]/div/strong/span\n",
    "\n",
    "# 개봉일\n",
    "# document.querySelector(\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(2) > li:nth-child(1) > div.box-contents > span.txt-info > strong\")\n",
    "#안 document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(11)\")\n",
    "# 딱그텍스트는 xpath뿐이네 //*[@id=\"contents\"]/div[1]/div[3]/ol[1]/li[1]/div[2]/span[1]/strong/text()\n",
    "\n",
    "# 평점(골든에그)\n",
    "# document.querySelector(\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(2) > li:nth-child(1) > div.box-contents > div > div > span.percent\")\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.score > div > span.percent\")\n",
    "# //*[@id=\"contents\"]/div[1]/div[3]/ol[1]/li[1]/div[2]/div/div/span[2]\n",
    "\n",
    "# 장르(어차피 얘는 안에서밖에 못긁음)\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dt:nth-child(6)\")\n",
    "# //*[@id=\"select_main\"]/div[1]/div[2]/div[3]/dl/dt[3]\n",
    "\n",
    "\n",
    "# 맨마지막\n",
    "# document.querySelector(\"#movie_more_container > li:nth-child(32) > div.box-contents > a > strong\")\n",
    "# document.querySelector(\"#movie_more_container > li:nth-child(31) > div.box-contents > a > strong\")\n",
    "# document.querySelector(\"#movie_more_container > li:nth-child(25) > div.box-contents > a > strong\")\n",
    "# document.querySelector(\"#movie_more_container > li:nth-child(17) > div.box-contents > a > strong\")\n",
    "# document.querySelector(\"#movie_more_container > li:nth-child(9) > div.box-contents > a > strong\")\n",
    "# document.querySelector(\"#movie_more_container > li:nth-child(1) > div.box-contents > a > strong\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "score_list = []\n",
    "    \n",
    "for i in range(1, 19) :       \n",
    "    a = ['li:nth-child(1)','li:nth-child(2)','li:nth-child(3)'][i]\n",
    "    data_list = soup.select(f'#contents > div.wrap-movie-chart > div.sect-movie-chart > {a} > li')\n",
    " \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "                               \n",
    "    movies_name = soup.select('li:nth-child(i) > div.box-contents > a > strong')[0].text\n",
    "# (\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(2) > li:nth-child(1) > div.box-contents > a > strong\")\n",
    "# (\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(2) > li:nth-child(2) > div.box-contents > a > strong\")\n",
    "# (\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(2) > li:nth-child(3) > div.box-contents > a > strong\")\n",
    "# (\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(3) > li:nth-child(1) > div.box-contents > a > strong\")    \n",
    "    booking_rate = soup.select('li:nth-child(i) > div.box-contents > div > strong > span')[0].text\n",
    "    date_open = soup.select('li:nth-child(i) > div.box-contents > span.txt-info > strong')[0].text \n",
    "    movies_score = soup.select('li:nth-child(i) > div.box-contents > div > div > span.percent')[0].text\n",
    "    \n",
    "    name_list.append(movies_name)\n",
    "    rate_list.append(booking_rate)\n",
    "    date_list.append(date_open)\n",
    "    score_list.append(movies_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['방탄소년단: 옛 투 컴 인 시네마',\n",
       " '아바타-물의 길',\n",
       " '더 퍼스트 슬램덩크',\n",
       " '교섭',\n",
       " '상견니',\n",
       " '유령',\n",
       " '메간',\n",
       " '다나카 1st 내한콘서트 - 생중계',\n",
       " '바빌론',\n",
       " '영웅',\n",
       " '오늘 밤, 세계에서 이 사랑이 사라진다 해도',\n",
       " '천룡팔부-교봉전',\n",
       " '뮤지컬 레드북',\n",
       " '장화신은 고양이-끝내주는 모험',\n",
       " '라인',\n",
       " '코코',\n",
       " '열여덟, 어른이 되는 나이',\n",
       " '엄마의 땅-그리샤와 숲의 주인',\n",
       " '이마 베프',\n",
       " '인사이드 아웃',\n",
       " '겨울왕국 2',\n",
       " '코코',\n",
       " '겨울왕국',\n",
       " '성스러운 거미',\n",
       " '주토피아',\n",
       " '400번의 구타',\n",
       " '쥴 앤 짐',\n",
       " '위너 2022 콘서트 더 서클 - 더 무비',\n",
       " '소울',\n",
       " '유랑의 달',\n",
       " '업',\n",
       " '단순한 열정',\n",
       " '캐리와 슈퍼콜라',\n",
       " '라일 라일 크로커다일',\n",
       " '3000년의 기다림',\n",
       " '[씨네뮤지엄] 빈센트 반 고흐, 세상에서 가장 빛나는 별',\n",
       " '본즈 앤 올',\n",
       " '엔칸토-마법의 세계',\n",
       " '[사이다경제] 평범한 직장인이 160억 대 자산가가 될 수 있었던 사연',\n",
       " '새를 사랑한 화가',\n",
       " '몬스터 하우스2-인비져블 피닉스',\n",
       " '메모리아',\n",
       " '어메이징 모리스',\n",
       " '[사이다경제] 안정적인 투자를 원한다면 돈의 흐름에 올라타라!',\n",
       " '[아트&다이닝]빈센트 반 고흐, 세상에서 가장 빛나는 별',\n",
       " '더 메뉴',\n",
       " '모아나',\n",
       " '[사이다경제] 은퇴 후 40년 동안 꼬박꼬박 현금으로 월급 받는 법',\n",
       " '[사이다경제] 오늘 배워 내일 바로 적용할 수 있는 주식 투자 이야기',\n",
       " '극장판 파워레인저 캡틴포스-지구를 위한 싸움',\n",
       " '시간을 꿈꾸는 소녀']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 너무 다 path가 제각각이라 이렇게 크게 긁는게 오히려 낫네 \n",
    "\n",
    "movies_name = soup.select('div > a > strong.title')\n",
    "name_list = []\n",
    "\n",
    "for name in movies_name:\n",
    "    name_list.append(name.text)\n",
    "    \n",
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['37.2%',\n",
       " '20.8%',\n",
       " '8.6%',\n",
       " '7.2%',\n",
       " '5.0%',\n",
       " '4.9%',\n",
       " '3.9%',\n",
       " '1.6%',\n",
       " '1.4%',\n",
       " '1.4%',\n",
       " '1.0%',\n",
       " '0.6%',\n",
       " '0.6%',\n",
       " '0.5%',\n",
       " '0.5%',\n",
       " '0.5%',\n",
       " '0.5%',\n",
       " '0.4%',\n",
       " '0.4%',\n",
       " '0.3%',\n",
       " '0.3%',\n",
       " '0.3%',\n",
       " '0.3%',\n",
       " '0.2%',\n",
       " '0.2%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.1%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%',\n",
       " '0.0%']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booking_rate = soup.select('div > strong > span')\n",
    "\n",
    "rate_list = []\n",
    "\n",
    "for rate in booking_rate:\n",
    "    rate_list.append(rate.text)\n",
    "    \n",
    "rate_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n                                2023.02.01 \\n                                개봉\\nD-3\\n',\n",
       " '\\n                                2022.12.14 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.04 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.18 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.25 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.18 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.25 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.29 \\n                                개봉\\n',\n",
       " '\\n                                2023.02.01 \\n                                개봉\\nD-3\\n',\n",
       " '\\n                                2022.12.21 \\n                                개봉\\n',\n",
       " '\\n                                2022.11.30 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.25 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.25 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.04 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.25 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.25 \\n                                재개봉\\n',\n",
       " '\\n                                2023.01.25 \\n                                개봉\\n',\n",
       " '\\n                                2023.01.25 \\n                                개봉\\n',\n",
       " '\\n                                2023.02.01 \\n                                개봉\\nD-3\\n',\n",
       " '                     2023.01.25                     재개봉 ',\n",
       " '                     2023.01.25                     재개봉 ',\n",
       " '                     2023.01.25                     재개봉 ',\n",
       " '                     2023.01.25                     재개봉 ',\n",
       " '                     2023.02.08                     개봉 D-10 ',\n",
       " '                     2023.01.25                     재개봉 ',\n",
       " '                     2023.01.25                     개봉 ',\n",
       " '                     2023.01.25                     개봉 ',\n",
       " '                     2023.01.25                     개봉 ',\n",
       " '                     2023.01.25                     재개봉 ',\n",
       " '                     2023.01.18                     개봉 ',\n",
       " '                     2023.01.25                     재개봉 ',\n",
       " '                     2023.02.01                     개봉 D-3 ',\n",
       " '                     2023.01.18                     개봉 ',\n",
       " '                     2023.01.18                     개봉 ',\n",
       " '                     2023.01.04                     개봉 ',\n",
       " '                     2023.02.21                     재개봉 D-23 ',\n",
       " '                     2022.11.30                     개봉 ',\n",
       " '                     2023.01.25                     재개봉 ',\n",
       " '                     2023.02.21                     개봉 D-23 ',\n",
       " '                     2023.01.25                     개봉 ',\n",
       " '                     2023.02.02                     개봉 D-4 ',\n",
       " '                     2022.12.29                     개봉 ',\n",
       " '                     2023.02.15                     개봉 D-17 ',\n",
       " '                     2023.01.31                     개봉 D-2 ',\n",
       " '                     2023.01.15                     개봉 ',\n",
       " '                     2022.12.07                     개봉 ',\n",
       " '                     2023.01.25                     재개봉 ',\n",
       " '                     2023.02.14                     개봉 D-16 ',\n",
       " '                     2023.02.07                     개봉 D-9 ',\n",
       " '                     2023.01.19                     개봉 ',\n",
       " '                     2023.01.11                     개봉 ']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_open = soup.select('span > strong')\n",
    "date_list = []\n",
    "\n",
    "for open in date_open:\n",
    "    date_list.append(open.text)             \n",
    "            \n",
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['99%',\n",
       " '96%',\n",
       " '97%',\n",
       " '81%',\n",
       " '81%',\n",
       " '83%',\n",
       " '88%',\n",
       " '?',\n",
       " '99%',\n",
       " '94%',\n",
       " '92%',\n",
       " '85%',\n",
       " '99%',\n",
       " '96%',\n",
       " '87%',\n",
       " '98%',\n",
       " '90%',\n",
       " '94%',\n",
       " '99%',\n",
       " '96%',\n",
       " '95%',\n",
       " '98%',\n",
       " '97%',\n",
       " '99%',\n",
       " '99%',\n",
       " '98%',\n",
       " '92%',\n",
       " '?',\n",
       " '97%',\n",
       " '88%',\n",
       " '98%',\n",
       " '99%',\n",
       " '93%',\n",
       " '94%',\n",
       " '90%',\n",
       " '?',\n",
       " '91%',\n",
       " '94%',\n",
       " '?',\n",
       " '?',\n",
       " '99%',\n",
       " '83%',\n",
       " '99%',\n",
       " '?',\n",
       " '?',\n",
       " '89%',\n",
       " '98%',\n",
       " '?',\n",
       " '?',\n",
       " '89%',\n",
       " '88%']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_score = soup.select('span.percent')\n",
    "score_list = []\n",
    "\n",
    "for score in movies_score:\n",
    "    score_list.append(score.text)\n",
    "    \n",
    "score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([name_list,rate_list,date_list,score_list,genre_list])\n",
    "CGV = data.T\n",
    "CGV.columns = ['영화제목', '예매율', '개봉일', '평점','장르X']\n",
    "CGV.to_csv('CGVTEST_.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지워보자 잡다한 문자열들 \n",
    "data = pd.read_csv('./CGVTEST_.csv')   # 먼저데이터읽어놓고\n",
    "\n",
    "# for i in enumerate(data): \n",
    "\n",
    "data['개봉일'] = data['개봉일'].str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('CGV 일단은성공.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgv = pd.read_csv('./CGV 일단은성공.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgv.to_excel('cgv찝찝성공.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "score_list = []\n",
    "genre_list = []\n",
    "\n",
    "for i in range(3) :\n",
    "    a = ['ol:nth-child(2)','ol:nth-child(3)','ol.list-more'][i]\n",
    "    data_list = soup.select(f'#contents > div.wrap-movie-chart > div.sect-movie-chart > {a} > li')\n",
    "    \n",
    "    for j in range(len(data_list)) :\n",
    "        driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div[1]/div[3]/ol[{i+1}]/li[{j+1}]/div[1]/a').send_keys(Keys.ENTER)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        movies_name = soup.select('div.sect-base-movie > div.box-contents > div.title > strong')[0].text\n",
    "        booking_rate = soup.select('div.box-contents > div.score > strong > span')[0].text\n",
    "        date_open = soup.select('div.box-contents > div.spec > dl > dd:nth-child(11)')[0].text \n",
    "# cgv는 안에서 긁는거 짜려면 일일이 하나하나 다 열어봐여되는거같다. 밖에서 긁도록하겠다. \n",
    "# 이미 이걸 알아내기까지 시간이 무척많이걸렸다. \n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(10)\") 3슬램덩크\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(12)\") 4교섭\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(11)\") 5상견니\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(11)\") 6유령\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(11)\") 7메간\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(9)\") 8다나카\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(11)\") 9바빌론\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(12)\") 10영웅\n",
    "# document.querySelector(\"#select_main > div.sect-base-movie > div.box-contents > div.spec > dl > dd:nth-child(11)\") 11\n",
    "\n",
    "\n",
    "        movies_score = soup.select('div.box-contents > div.score > div > span.percent')[0].text\n",
    "        movies_genre = soup.select('div.box-contents > div.spec > dl > dt:nth-child(6)')[0].text\n",
    "        \n",
    "        driver.back()\n",
    "        time.sleep(2)\n",
    "\n",
    "        name_list.append(movies_name)\n",
    "        rate_list.append(booking_rate)\n",
    "        date_list.append(date_open)\n",
    "        score_list.append(movies_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "name_list = []\n",
    "rate_list = []\n",
    "date_list = []\n",
    "score_list = []\n",
    "    \n",
    "for i in range(1, ) :       \n",
    "\n",
    " \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "                               \n",
    "    movies_name = soup.select('li:nth-child(1) > div.box-contents > a > strong')[i-1].text\n",
    "    booking_rate = soup.select('li:nth-child(1) > div.box-contents > div > strong > span')[i-1].text\n",
    "    date_open = soup.select('li:nth-child(1) > div.box-contents > span.txt-info > strong')[i-1].text \n",
    "    movies_score = soup.select('li:nth-child(1) > div.box-contents > div > div > span.percent')[i-1].text\n",
    "    \n",
    "    name_list.append(movies_name)\n",
    "    rate_list.append(booking_rate)\n",
    "    date_list.append(date_open)\n",
    "    score_list.append(movies_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한꺼풀 들어갈때\n",
    "# document.querySelector(\"#contents > div.wrap-movie-chart > div.sect-movie-chart > ol:nth-child(2) > li:nth-child(2) > div.box-image > a\")\n",
    "# //*[@id=\"contents\"]/div[1]/div[3]/ol[1]/li[2]/div[1]/a\n",
    "# //*[@id=\"contents\"]/div[1]/div[3]/ol[1]/li[3]/div[1]/a\n",
    "\n",
    "genre_list = []\n",
    "\n",
    "\n",
    "for i in range(1, 91):\n",
    "\n",
    "    driver.find_element(By.XPATH,f'//*[@id=\"contents\"]/div[1]/div[3]/ol[1]/li[{i}]/div[1]/a').send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "    movies_genre = soup.select('div.movie-info.infoContent > div > p:nth-child(2)')[0].text\n",
    "\n",
    "    driver.back()\n",
    "    time.sleep(2)\n",
    "\n",
    "    genre_list.append(movies_genre)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bef482946e96f88098b5efcaeffc69c0510dfd6df865bc1fb622e8f0f6e91f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
